{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "\n",
    "def reduce_mem_usage(df, use_float16=False):\n",
    "    \"\"\"\n",
    "    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    \n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n",
    "            continue\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
    "    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 65.85 MB\n",
      "Memory usage after optimization is: 26.09 MB\n",
      "Decreased by 60.4%\n"
     ]
    }
   ],
   "source": [
    "df_merge = reduce_mem_usage(pd.read_csv('./output/final_merge.csv'))\n",
    "categorical_features = ['airconditioningtypeid', 'hashottuborspa', 'heatingorsystemtypeid', \n",
    "                       'pooltypeid2', 'propertylandusetypeid', 'fips', 'regionidcounty', \n",
    "                       'buildingqualitytypeid_fill', 'regionidcity_fill', 'year', \n",
    "                       'regionidneighborhood_fill', 'taxdelinquencyflag']\n",
    "df_drop = df_merge.drop_duplicates(subset = ['parcelid', 'logerror'])\n",
    "df_drop = df_drop[ df_drop.logerror > -0.4 ]\n",
    "df_drop = df_drop[ df_drop.logerror < 0.419 ]\n",
    "df_drop = df_drop.reset_index(drop = True)\n",
    "target = df_drop.logerror\n",
    "features = df_drop.drop(['logerror'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop = pd.read_csv('./output/outlier_remove.csv')\n",
    "categorical_features = ['airconditioningtypeid', 'hashottuborspa', 'heatingorsystemtypeid', \n",
    "                       'pooltypeid2', 'propertylandusetypeid', 'fips', 'regionidcounty', \n",
    "                       'buildingqualitytypeid_fill', 'regionidcity_fill', 'year', \n",
    "                       'regionidneighborhood_fill', 'taxdelinquencyflag']\n",
    "target = df_drop.logerror\n",
    "features = df_drop.drop(['logerror'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[25]\ttraining's rmse: 0.0822919\tvalid_1's rmse: 0.0858488\n",
      "[50]\ttraining's rmse: 0.082208\tvalid_1's rmse: 0.0857774\n",
      "[75]\ttraining's rmse: 0.082126\tvalid_1's rmse: 0.0857063\n",
      "[100]\ttraining's rmse: 0.0820519\tvalid_1's rmse: 0.0856467\n",
      "[125]\ttraining's rmse: 0.0819758\tvalid_1's rmse: 0.0855874\n",
      "[150]\ttraining's rmse: 0.0819027\tvalid_1's rmse: 0.0855269\n",
      "[175]\ttraining's rmse: 0.081843\tvalid_1's rmse: 0.085481\n",
      "[200]\ttraining's rmse: 0.0817786\tvalid_1's rmse: 0.0854322\n",
      "[225]\ttraining's rmse: 0.0817165\tvalid_1's rmse: 0.0853869\n",
      "[250]\ttraining's rmse: 0.0816632\tvalid_1's rmse: 0.0853469\n",
      "[275]\ttraining's rmse: 0.0816131\tvalid_1's rmse: 0.0853098\n",
      "[300]\ttraining's rmse: 0.0815619\tvalid_1's rmse: 0.0852739\n",
      "[325]\ttraining's rmse: 0.0815119\tvalid_1's rmse: 0.0852399\n",
      "[350]\ttraining's rmse: 0.0814649\tvalid_1's rmse: 0.0852101\n",
      "[375]\ttraining's rmse: 0.0814274\tvalid_1's rmse: 0.085183\n",
      "[400]\ttraining's rmse: 0.081385\tvalid_1's rmse: 0.0851572\n",
      "[425]\ttraining's rmse: 0.0813469\tvalid_1's rmse: 0.08513\n",
      "[450]\ttraining's rmse: 0.0813076\tvalid_1's rmse: 0.0851028\n",
      "[475]\ttraining's rmse: 0.0812782\tvalid_1's rmse: 0.0850804\n",
      "[500]\ttraining's rmse: 0.0812517\tvalid_1's rmse: 0.0850614\n",
      "[525]\ttraining's rmse: 0.0812179\tvalid_1's rmse: 0.0850432\n",
      "[550]\ttraining's rmse: 0.0811832\tvalid_1's rmse: 0.0850245\n",
      "[575]\ttraining's rmse: 0.081153\tvalid_1's rmse: 0.0850085\n",
      "[600]\ttraining's rmse: 0.0811243\tvalid_1's rmse: 0.0849918\n",
      "[625]\ttraining's rmse: 0.0811011\tvalid_1's rmse: 0.0849755\n",
      "[650]\ttraining's rmse: 0.0810724\tvalid_1's rmse: 0.0849592\n",
      "[675]\ttraining's rmse: 0.0810437\tvalid_1's rmse: 0.0849441\n",
      "[700]\ttraining's rmse: 0.0810178\tvalid_1's rmse: 0.0849303\n",
      "[725]\ttraining's rmse: 0.0809944\tvalid_1's rmse: 0.0849185\n",
      "[750]\ttraining's rmse: 0.0809719\tvalid_1's rmse: 0.0849068\n",
      "[775]\ttraining's rmse: 0.0809529\tvalid_1's rmse: 0.0848946\n",
      "[800]\ttraining's rmse: 0.0809287\tvalid_1's rmse: 0.0848832\n",
      "[825]\ttraining's rmse: 0.0809071\tvalid_1's rmse: 0.0848721\n",
      "[850]\ttraining's rmse: 0.0808867\tvalid_1's rmse: 0.0848638\n",
      "[875]\ttraining's rmse: 0.0808677\tvalid_1's rmse: 0.0848544\n",
      "[900]\ttraining's rmse: 0.0808452\tvalid_1's rmse: 0.0848451\n",
      "[925]\ttraining's rmse: 0.0808267\tvalid_1's rmse: 0.0848375\n",
      "[950]\ttraining's rmse: 0.0808085\tvalid_1's rmse: 0.084829\n",
      "[975]\ttraining's rmse: 0.0807924\tvalid_1's rmse: 0.0848209\n",
      "[1000]\ttraining's rmse: 0.0807759\tvalid_1's rmse: 0.0848131\n",
      "[1025]\ttraining's rmse: 0.0807579\tvalid_1's rmse: 0.0848055\n",
      "[1050]\ttraining's rmse: 0.0807416\tvalid_1's rmse: 0.0847973\n",
      "[1075]\ttraining's rmse: 0.0807267\tvalid_1's rmse: 0.0847917\n",
      "[1100]\ttraining's rmse: 0.0807139\tvalid_1's rmse: 0.0847866\n",
      "[1125]\ttraining's rmse: 0.0807015\tvalid_1's rmse: 0.0847814\n",
      "[1150]\ttraining's rmse: 0.0806845\tvalid_1's rmse: 0.0847752\n",
      "[1175]\ttraining's rmse: 0.0806727\tvalid_1's rmse: 0.084771\n",
      "[1200]\ttraining's rmse: 0.0806615\tvalid_1's rmse: 0.0847649\n",
      "[1225]\ttraining's rmse: 0.0806482\tvalid_1's rmse: 0.08476\n",
      "[1250]\ttraining's rmse: 0.0806364\tvalid_1's rmse: 0.0847566\n",
      "[1275]\ttraining's rmse: 0.0806217\tvalid_1's rmse: 0.0847521\n",
      "[1300]\ttraining's rmse: 0.0806112\tvalid_1's rmse: 0.0847479\n",
      "[1325]\ttraining's rmse: 0.0805998\tvalid_1's rmse: 0.0847441\n",
      "[1350]\ttraining's rmse: 0.0805887\tvalid_1's rmse: 0.0847406\n",
      "[1375]\ttraining's rmse: 0.0805787\tvalid_1's rmse: 0.0847373\n",
      "[1400]\ttraining's rmse: 0.0805694\tvalid_1's rmse: 0.0847335\n",
      "[1425]\ttraining's rmse: 0.0805585\tvalid_1's rmse: 0.0847315\n",
      "[1450]\ttraining's rmse: 0.0805486\tvalid_1's rmse: 0.0847283\n",
      "[1475]\ttraining's rmse: 0.0805398\tvalid_1's rmse: 0.084725\n",
      "[1500]\ttraining's rmse: 0.0805311\tvalid_1's rmse: 0.0847228\n",
      "[1525]\ttraining's rmse: 0.080523\tvalid_1's rmse: 0.0847199\n",
      "[1550]\ttraining's rmse: 0.0805151\tvalid_1's rmse: 0.0847181\n",
      "[1575]\ttraining's rmse: 0.0805083\tvalid_1's rmse: 0.0847159\n",
      "[1600]\ttraining's rmse: 0.0805019\tvalid_1's rmse: 0.0847146\n",
      "[1625]\ttraining's rmse: 0.0804946\tvalid_1's rmse: 0.0847115\n",
      "[1650]\ttraining's rmse: 0.0804872\tvalid_1's rmse: 0.0847085\n",
      "[1675]\ttraining's rmse: 0.0804804\tvalid_1's rmse: 0.0847064\n",
      "[1700]\ttraining's rmse: 0.0804758\tvalid_1's rmse: 0.0847043\n",
      "[1725]\ttraining's rmse: 0.0804718\tvalid_1's rmse: 0.0847032\n",
      "[1750]\ttraining's rmse: 0.0804649\tvalid_1's rmse: 0.0847005\n",
      "[1775]\ttraining's rmse: 0.0804583\tvalid_1's rmse: 0.0846995\n",
      "[1800]\ttraining's rmse: 0.0804526\tvalid_1's rmse: 0.0846978\n",
      "[1825]\ttraining's rmse: 0.0804479\tvalid_1's rmse: 0.0846963\n",
      "[1850]\ttraining's rmse: 0.0804437\tvalid_1's rmse: 0.0846938\n",
      "[1875]\ttraining's rmse: 0.0804389\tvalid_1's rmse: 0.0846922\n",
      "[1900]\ttraining's rmse: 0.0804345\tvalid_1's rmse: 0.0846909\n",
      "[1925]\ttraining's rmse: 0.0804294\tvalid_1's rmse: 0.0846888\n",
      "[1950]\ttraining's rmse: 0.0804259\tvalid_1's rmse: 0.084687\n",
      "[1975]\ttraining's rmse: 0.0804224\tvalid_1's rmse: 0.0846856\n",
      "[2000]\ttraining's rmse: 0.0804164\tvalid_1's rmse: 0.0846841\n",
      "[2025]\ttraining's rmse: 0.0804133\tvalid_1's rmse: 0.0846823\n",
      "[2050]\ttraining's rmse: 0.0804102\tvalid_1's rmse: 0.0846813\n",
      "[2075]\ttraining's rmse: 0.0804066\tvalid_1's rmse: 0.0846804\n",
      "[2100]\ttraining's rmse: 0.0804043\tvalid_1's rmse: 0.0846796\n",
      "[2125]\ttraining's rmse: 0.0804011\tvalid_1's rmse: 0.0846782\n",
      "[2150]\ttraining's rmse: 0.0803979\tvalid_1's rmse: 0.0846776\n",
      "[2175]\ttraining's rmse: 0.0803952\tvalid_1's rmse: 0.0846769\n",
      "[2200]\ttraining's rmse: 0.080392\tvalid_1's rmse: 0.0846757\n",
      "[2225]\ttraining's rmse: 0.0803891\tvalid_1's rmse: 0.0846744\n",
      "[2250]\ttraining's rmse: 0.0803859\tvalid_1's rmse: 0.0846734\n",
      "[2275]\ttraining's rmse: 0.0803835\tvalid_1's rmse: 0.0846728\n",
      "[2300]\ttraining's rmse: 0.0803808\tvalid_1's rmse: 0.0846714\n",
      "[2325]\ttraining's rmse: 0.0803771\tvalid_1's rmse: 0.0846708\n",
      "[2350]\ttraining's rmse: 0.0803751\tvalid_1's rmse: 0.0846704\n",
      "[2375]\ttraining's rmse: 0.0803722\tvalid_1's rmse: 0.0846698\n",
      "[2400]\ttraining's rmse: 0.0803695\tvalid_1's rmse: 0.084669\n",
      "[2425]\ttraining's rmse: 0.080368\tvalid_1's rmse: 0.0846684\n",
      "[2450]\ttraining's rmse: 0.0803656\tvalid_1's rmse: 0.0846673\n",
      "[2475]\ttraining's rmse: 0.0803631\tvalid_1's rmse: 0.0846668\n",
      "[2500]\ttraining's rmse: 0.0803609\tvalid_1's rmse: 0.0846665\n",
      "[2525]\ttraining's rmse: 0.0803583\tvalid_1's rmse: 0.0846655\n",
      "[2550]\ttraining's rmse: 0.0803564\tvalid_1's rmse: 0.0846649\n",
      "[2575]\ttraining's rmse: 0.0803551\tvalid_1's rmse: 0.084664\n",
      "[2600]\ttraining's rmse: 0.0803533\tvalid_1's rmse: 0.0846637\n",
      "[2625]\ttraining's rmse: 0.0803514\tvalid_1's rmse: 0.0846628\n",
      "[2650]\ttraining's rmse: 0.0803499\tvalid_1's rmse: 0.0846626\n",
      "[2675]\ttraining's rmse: 0.0803481\tvalid_1's rmse: 0.0846617\n",
      "[2700]\ttraining's rmse: 0.0803472\tvalid_1's rmse: 0.0846613\n",
      "[2725]\ttraining's rmse: 0.0803454\tvalid_1's rmse: 0.0846605\n",
      "[2750]\ttraining's rmse: 0.0803439\tvalid_1's rmse: 0.0846598\n",
      "[2775]\ttraining's rmse: 0.0803422\tvalid_1's rmse: 0.0846593\n",
      "[2800]\ttraining's rmse: 0.0803409\tvalid_1's rmse: 0.0846589\n",
      "[2825]\ttraining's rmse: 0.0803392\tvalid_1's rmse: 0.0846588\n",
      "[2850]\ttraining's rmse: 0.0803373\tvalid_1's rmse: 0.0846588\n",
      "[2875]\ttraining's rmse: 0.0803352\tvalid_1's rmse: 0.0846585\n",
      "[2900]\ttraining's rmse: 0.0803343\tvalid_1's rmse: 0.0846587\n",
      "[2925]\ttraining's rmse: 0.0803336\tvalid_1's rmse: 0.0846582\n",
      "[2950]\ttraining's rmse: 0.0803325\tvalid_1's rmse: 0.0846579\n",
      "[2975]\ttraining's rmse: 0.0803304\tvalid_1's rmse: 0.0846574\n",
      "[3000]\ttraining's rmse: 0.0803287\tvalid_1's rmse: 0.0846575\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2999]\ttraining's rmse: 0.0803287\tvalid_1's rmse: 0.0846575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/home/ningzesun/.local/lib/python3.6/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[25]\ttraining's rmse: 0.0833887\tvalid_1's rmse: 0.0814841\n",
      "[50]\ttraining's rmse: 0.0832997\tvalid_1's rmse: 0.0814188\n",
      "[75]\ttraining's rmse: 0.0832124\tvalid_1's rmse: 0.0813543\n",
      "[100]\ttraining's rmse: 0.0831332\tvalid_1's rmse: 0.0812984\n",
      "[125]\ttraining's rmse: 0.0830533\tvalid_1's rmse: 0.0812436\n",
      "[150]\ttraining's rmse: 0.0829747\tvalid_1's rmse: 0.0811869\n",
      "[175]\ttraining's rmse: 0.0829106\tvalid_1's rmse: 0.0811416\n",
      "[200]\ttraining's rmse: 0.0828424\tvalid_1's rmse: 0.0810948\n",
      "[225]\ttraining's rmse: 0.0827745\tvalid_1's rmse: 0.0810511\n",
      "[250]\ttraining's rmse: 0.0827182\tvalid_1's rmse: 0.0810134\n",
      "[275]\ttraining's rmse: 0.0826644\tvalid_1's rmse: 0.0809791\n",
      "[300]\ttraining's rmse: 0.0826118\tvalid_1's rmse: 0.0809443\n",
      "[325]\ttraining's rmse: 0.0825618\tvalid_1's rmse: 0.0809132\n",
      "[350]\ttraining's rmse: 0.0825104\tvalid_1's rmse: 0.0808838\n",
      "[375]\ttraining's rmse: 0.082468\tvalid_1's rmse: 0.0808573\n",
      "[400]\ttraining's rmse: 0.0824209\tvalid_1's rmse: 0.0808309\n",
      "[425]\ttraining's rmse: 0.0823788\tvalid_1's rmse: 0.080806\n",
      "[450]\ttraining's rmse: 0.0823363\tvalid_1's rmse: 0.0807814\n",
      "[475]\ttraining's rmse: 0.082303\tvalid_1's rmse: 0.0807592\n",
      "[500]\ttraining's rmse: 0.0822744\tvalid_1's rmse: 0.0807408\n",
      "[525]\ttraining's rmse: 0.0822369\tvalid_1's rmse: 0.0807211\n",
      "[550]\ttraining's rmse: 0.0822008\tvalid_1's rmse: 0.0807031\n",
      "[575]\ttraining's rmse: 0.0821694\tvalid_1's rmse: 0.0806886\n",
      "[600]\ttraining's rmse: 0.0821377\tvalid_1's rmse: 0.0806726\n",
      "[625]\ttraining's rmse: 0.0821124\tvalid_1's rmse: 0.0806596\n",
      "[650]\ttraining's rmse: 0.0820799\tvalid_1's rmse: 0.0806439\n",
      "[675]\ttraining's rmse: 0.0820482\tvalid_1's rmse: 0.08063\n",
      "[700]\ttraining's rmse: 0.082021\tvalid_1's rmse: 0.0806165\n",
      "[725]\ttraining's rmse: 0.0819965\tvalid_1's rmse: 0.0806053\n",
      "[750]\ttraining's rmse: 0.0819709\tvalid_1's rmse: 0.0805936\n",
      "[775]\ttraining's rmse: 0.0819497\tvalid_1's rmse: 0.0805824\n",
      "[800]\ttraining's rmse: 0.081924\tvalid_1's rmse: 0.0805724\n",
      "[825]\ttraining's rmse: 0.0819009\tvalid_1's rmse: 0.0805625\n",
      "[850]\ttraining's rmse: 0.0818804\tvalid_1's rmse: 0.0805548\n",
      "[875]\ttraining's rmse: 0.0818586\tvalid_1's rmse: 0.0805463\n",
      "[900]\ttraining's rmse: 0.0818365\tvalid_1's rmse: 0.08054\n",
      "[925]\ttraining's rmse: 0.0818144\tvalid_1's rmse: 0.0805313\n",
      "[950]\ttraining's rmse: 0.081796\tvalid_1's rmse: 0.0805236\n",
      "[975]\ttraining's rmse: 0.0817774\tvalid_1's rmse: 0.0805164\n",
      "[1000]\ttraining's rmse: 0.08176\tvalid_1's rmse: 0.0805098\n",
      "[1025]\ttraining's rmse: 0.0817391\tvalid_1's rmse: 0.0805022\n",
      "[1050]\ttraining's rmse: 0.0817212\tvalid_1's rmse: 0.0804949\n",
      "[1075]\ttraining's rmse: 0.0817045\tvalid_1's rmse: 0.0804892\n",
      "[1100]\ttraining's rmse: 0.0816906\tvalid_1's rmse: 0.080485\n",
      "[1125]\ttraining's rmse: 0.0816763\tvalid_1's rmse: 0.0804791\n",
      "[1150]\ttraining's rmse: 0.0816613\tvalid_1's rmse: 0.0804744\n",
      "[1175]\ttraining's rmse: 0.0816497\tvalid_1's rmse: 0.0804708\n",
      "[1200]\ttraining's rmse: 0.0816366\tvalid_1's rmse: 0.0804656\n",
      "[1225]\ttraining's rmse: 0.0816233\tvalid_1's rmse: 0.0804623\n",
      "[1250]\ttraining's rmse: 0.081611\tvalid_1's rmse: 0.0804589\n",
      "[1275]\ttraining's rmse: 0.0815974\tvalid_1's rmse: 0.0804555\n",
      "[1300]\ttraining's rmse: 0.0815861\tvalid_1's rmse: 0.080452\n",
      "[1325]\ttraining's rmse: 0.0815742\tvalid_1's rmse: 0.0804477\n",
      "[1350]\ttraining's rmse: 0.0815626\tvalid_1's rmse: 0.0804447\n",
      "[1375]\ttraining's rmse: 0.0815515\tvalid_1's rmse: 0.0804413\n",
      "[1400]\ttraining's rmse: 0.0815413\tvalid_1's rmse: 0.0804381\n",
      "[1425]\ttraining's rmse: 0.0815308\tvalid_1's rmse: 0.0804353\n",
      "[1450]\ttraining's rmse: 0.0815205\tvalid_1's rmse: 0.0804331\n",
      "[1475]\ttraining's rmse: 0.0815119\tvalid_1's rmse: 0.0804305\n",
      "[1500]\ttraining's rmse: 0.081504\tvalid_1's rmse: 0.0804282\n",
      "[1525]\ttraining's rmse: 0.0814944\tvalid_1's rmse: 0.0804261\n",
      "[1550]\ttraining's rmse: 0.0814857\tvalid_1's rmse: 0.0804233\n",
      "[1575]\ttraining's rmse: 0.0814778\tvalid_1's rmse: 0.0804211\n",
      "[1600]\ttraining's rmse: 0.0814712\tvalid_1's rmse: 0.0804198\n",
      "[1625]\ttraining's rmse: 0.0814616\tvalid_1's rmse: 0.0804174\n",
      "[1650]\ttraining's rmse: 0.0814551\tvalid_1's rmse: 0.0804158\n",
      "[1675]\ttraining's rmse: 0.0814506\tvalid_1's rmse: 0.0804144\n",
      "[1700]\ttraining's rmse: 0.0814466\tvalid_1's rmse: 0.0804131\n",
      "[1725]\ttraining's rmse: 0.0814406\tvalid_1's rmse: 0.0804116\n",
      "[1750]\ttraining's rmse: 0.0814336\tvalid_1's rmse: 0.0804092\n",
      "[1775]\ttraining's rmse: 0.0814295\tvalid_1's rmse: 0.0804087\n",
      "[1800]\ttraining's rmse: 0.0814247\tvalid_1's rmse: 0.0804077\n",
      "[1825]\ttraining's rmse: 0.0814194\tvalid_1's rmse: 0.0804068\n",
      "[1850]\ttraining's rmse: 0.0814156\tvalid_1's rmse: 0.0804057\n",
      "[1875]\ttraining's rmse: 0.0814111\tvalid_1's rmse: 0.0804048\n",
      "[1900]\ttraining's rmse: 0.0814064\tvalid_1's rmse: 0.0804044\n",
      "[1925]\ttraining's rmse: 0.0814028\tvalid_1's rmse: 0.080404\n",
      "[1950]\ttraining's rmse: 0.0814002\tvalid_1's rmse: 0.0804031\n",
      "[1975]\ttraining's rmse: 0.0813968\tvalid_1's rmse: 0.0804017\n",
      "[2000]\ttraining's rmse: 0.0813934\tvalid_1's rmse: 0.080401\n",
      "[2025]\ttraining's rmse: 0.0813901\tvalid_1's rmse: 0.0804\n",
      "[2050]\ttraining's rmse: 0.081387\tvalid_1's rmse: 0.0803994\n",
      "[2075]\ttraining's rmse: 0.0813843\tvalid_1's rmse: 0.0803985\n",
      "[2100]\ttraining's rmse: 0.0813821\tvalid_1's rmse: 0.0803975\n",
      "[2125]\ttraining's rmse: 0.0813802\tvalid_1's rmse: 0.080397\n",
      "[2150]\ttraining's rmse: 0.081375\tvalid_1's rmse: 0.0803965\n",
      "[2175]\ttraining's rmse: 0.0813713\tvalid_1's rmse: 0.0803961\n",
      "[2200]\ttraining's rmse: 0.0813691\tvalid_1's rmse: 0.0803959\n",
      "[2225]\ttraining's rmse: 0.0813669\tvalid_1's rmse: 0.0803955\n",
      "[2250]\ttraining's rmse: 0.081365\tvalid_1's rmse: 0.0803951\n",
      "[2275]\ttraining's rmse: 0.0813617\tvalid_1's rmse: 0.0803947\n",
      "[2300]\ttraining's rmse: 0.0813599\tvalid_1's rmse: 0.0803939\n",
      "[2325]\ttraining's rmse: 0.0813554\tvalid_1's rmse: 0.0803935\n",
      "[2350]\ttraining's rmse: 0.0813521\tvalid_1's rmse: 0.0803931\n",
      "[2375]\ttraining's rmse: 0.0813499\tvalid_1's rmse: 0.0803932\n",
      "[2400]\ttraining's rmse: 0.0813465\tvalid_1's rmse: 0.0803926\n",
      "[2425]\ttraining's rmse: 0.0813446\tvalid_1's rmse: 0.0803926\n",
      "[2450]\ttraining's rmse: 0.0813434\tvalid_1's rmse: 0.0803924\n",
      "[2475]\ttraining's rmse: 0.0813417\tvalid_1's rmse: 0.0803922\n",
      "[2500]\ttraining's rmse: 0.0813402\tvalid_1's rmse: 0.0803917\n",
      "[2525]\ttraining's rmse: 0.0813378\tvalid_1's rmse: 0.0803913\n",
      "[2550]\ttraining's rmse: 0.0813355\tvalid_1's rmse: 0.0803907\n",
      "[2575]\ttraining's rmse: 0.0813345\tvalid_1's rmse: 0.0803907\n",
      "[2600]\ttraining's rmse: 0.0813326\tvalid_1's rmse: 0.0803902\n",
      "[2625]\ttraining's rmse: 0.0813317\tvalid_1's rmse: 0.0803899\n",
      "[2650]\ttraining's rmse: 0.0813297\tvalid_1's rmse: 0.0803896\n",
      "[2675]\ttraining's rmse: 0.0813284\tvalid_1's rmse: 0.0803892\n",
      "[2700]\ttraining's rmse: 0.0813267\tvalid_1's rmse: 0.0803891\n",
      "[2725]\ttraining's rmse: 0.0813246\tvalid_1's rmse: 0.0803891\n",
      "[2750]\ttraining's rmse: 0.0813234\tvalid_1's rmse: 0.080389\n",
      "[2775]\ttraining's rmse: 0.0813216\tvalid_1's rmse: 0.0803888\n",
      "[2800]\ttraining's rmse: 0.0813209\tvalid_1's rmse: 0.0803887\n",
      "[2825]\ttraining's rmse: 0.0813199\tvalid_1's rmse: 0.080389\n",
      "[2850]\ttraining's rmse: 0.0813185\tvalid_1's rmse: 0.0803893\n",
      "Early stopping, best iteration is:\n",
      "[2810]\ttraining's rmse: 0.0813208\tvalid_1's rmse: 0.0803887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/home/ningzesun/.local/lib/python3.6/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[25]\ttraining's rmse: 0.0826851\tvalid_1's rmse: 0.0843388\n",
      "[50]\ttraining's rmse: 0.0825915\tvalid_1's rmse: 0.0842823\n",
      "[75]\ttraining's rmse: 0.0825016\tvalid_1's rmse: 0.0842285\n",
      "[100]\ttraining's rmse: 0.08242\tvalid_1's rmse: 0.0841807\n",
      "[125]\ttraining's rmse: 0.0823358\tvalid_1's rmse: 0.0841343\n",
      "[150]\ttraining's rmse: 0.0822545\tvalid_1's rmse: 0.0840882\n",
      "[175]\ttraining's rmse: 0.0821889\tvalid_1's rmse: 0.0840517\n",
      "[200]\ttraining's rmse: 0.0821167\tvalid_1's rmse: 0.0840157\n",
      "[225]\ttraining's rmse: 0.0820487\tvalid_1's rmse: 0.083982\n",
      "[250]\ttraining's rmse: 0.0819914\tvalid_1's rmse: 0.0839502\n",
      "[275]\ttraining's rmse: 0.0819362\tvalid_1's rmse: 0.0839206\n",
      "[300]\ttraining's rmse: 0.0818799\tvalid_1's rmse: 0.0838932\n",
      "[325]\ttraining's rmse: 0.0818264\tvalid_1's rmse: 0.083867\n",
      "[350]\ttraining's rmse: 0.0817771\tvalid_1's rmse: 0.0838447\n",
      "[375]\ttraining's rmse: 0.0817356\tvalid_1's rmse: 0.0838241\n",
      "[400]\ttraining's rmse: 0.0816907\tvalid_1's rmse: 0.083805\n",
      "[425]\ttraining's rmse: 0.08165\tvalid_1's rmse: 0.0837863\n",
      "[450]\ttraining's rmse: 0.0816094\tvalid_1's rmse: 0.083767\n",
      "[475]\ttraining's rmse: 0.0815749\tvalid_1's rmse: 0.0837497\n",
      "[500]\ttraining's rmse: 0.0815461\tvalid_1's rmse: 0.0837341\n",
      "[525]\ttraining's rmse: 0.0815099\tvalid_1's rmse: 0.0837205\n",
      "[550]\ttraining's rmse: 0.0814734\tvalid_1's rmse: 0.0837059\n",
      "[575]\ttraining's rmse: 0.0814422\tvalid_1's rmse: 0.0836918\n",
      "[600]\ttraining's rmse: 0.0814119\tvalid_1's rmse: 0.0836792\n",
      "[625]\ttraining's rmse: 0.0813872\tvalid_1's rmse: 0.0836668\n",
      "[650]\ttraining's rmse: 0.0813565\tvalid_1's rmse: 0.0836549\n",
      "[675]\ttraining's rmse: 0.0813255\tvalid_1's rmse: 0.0836435\n",
      "[700]\ttraining's rmse: 0.0812977\tvalid_1's rmse: 0.0836321\n",
      "[725]\ttraining's rmse: 0.0812722\tvalid_1's rmse: 0.0836215\n",
      "[750]\ttraining's rmse: 0.081248\tvalid_1's rmse: 0.083611\n",
      "[775]\ttraining's rmse: 0.0812262\tvalid_1's rmse: 0.0836015\n",
      "[800]\ttraining's rmse: 0.0812021\tvalid_1's rmse: 0.0835939\n",
      "[825]\ttraining's rmse: 0.0811807\tvalid_1's rmse: 0.0835862\n",
      "[850]\ttraining's rmse: 0.0811587\tvalid_1's rmse: 0.0835783\n",
      "[875]\ttraining's rmse: 0.0811387\tvalid_1's rmse: 0.0835709\n",
      "[900]\ttraining's rmse: 0.0811165\tvalid_1's rmse: 0.083564\n",
      "[925]\ttraining's rmse: 0.0810945\tvalid_1's rmse: 0.0835572\n",
      "[950]\ttraining's rmse: 0.0810765\tvalid_1's rmse: 0.0835509\n",
      "[975]\ttraining's rmse: 0.0810585\tvalid_1's rmse: 0.0835443\n",
      "[1000]\ttraining's rmse: 0.081042\tvalid_1's rmse: 0.0835383\n",
      "[1025]\ttraining's rmse: 0.0810218\tvalid_1's rmse: 0.0835328\n",
      "[1050]\ttraining's rmse: 0.081006\tvalid_1's rmse: 0.0835267\n",
      "[1075]\ttraining's rmse: 0.0809891\tvalid_1's rmse: 0.0835224\n",
      "[1100]\ttraining's rmse: 0.0809765\tvalid_1's rmse: 0.083518\n",
      "[1125]\ttraining's rmse: 0.0809618\tvalid_1's rmse: 0.0835134\n",
      "[1150]\ttraining's rmse: 0.0809455\tvalid_1's rmse: 0.0835099\n",
      "[1175]\ttraining's rmse: 0.0809331\tvalid_1's rmse: 0.0835059\n",
      "[1200]\ttraining's rmse: 0.0809212\tvalid_1's rmse: 0.0835013\n",
      "[1225]\ttraining's rmse: 0.0809071\tvalid_1's rmse: 0.0834978\n",
      "[1250]\ttraining's rmse: 0.0808948\tvalid_1's rmse: 0.0834944\n",
      "[1275]\ttraining's rmse: 0.0808831\tvalid_1's rmse: 0.0834909\n",
      "[1300]\ttraining's rmse: 0.0808712\tvalid_1's rmse: 0.0834869\n",
      "[1325]\ttraining's rmse: 0.0808595\tvalid_1's rmse: 0.0834845\n",
      "[1350]\ttraining's rmse: 0.0808474\tvalid_1's rmse: 0.0834826\n",
      "[1375]\ttraining's rmse: 0.0808369\tvalid_1's rmse: 0.0834804\n",
      "[1400]\ttraining's rmse: 0.0808281\tvalid_1's rmse: 0.0834767\n",
      "[1425]\ttraining's rmse: 0.0808163\tvalid_1's rmse: 0.0834744\n",
      "[1450]\ttraining's rmse: 0.0808057\tvalid_1's rmse: 0.083473\n",
      "[1475]\ttraining's rmse: 0.0807971\tvalid_1's rmse: 0.083471\n",
      "[1500]\ttraining's rmse: 0.0807866\tvalid_1's rmse: 0.083469\n",
      "[1525]\ttraining's rmse: 0.0807778\tvalid_1's rmse: 0.0834666\n",
      "[1550]\ttraining's rmse: 0.0807701\tvalid_1's rmse: 0.0834654\n",
      "[1575]\ttraining's rmse: 0.0807622\tvalid_1's rmse: 0.0834632\n",
      "[1600]\ttraining's rmse: 0.080755\tvalid_1's rmse: 0.0834626\n",
      "[1625]\ttraining's rmse: 0.0807464\tvalid_1's rmse: 0.0834609\n",
      "[1650]\ttraining's rmse: 0.0807391\tvalid_1's rmse: 0.0834603\n",
      "[1675]\ttraining's rmse: 0.0807334\tvalid_1's rmse: 0.0834593\n",
      "[1700]\ttraining's rmse: 0.0807275\tvalid_1's rmse: 0.0834571\n",
      "[1725]\ttraining's rmse: 0.0807216\tvalid_1's rmse: 0.0834559\n",
      "[1750]\ttraining's rmse: 0.0807147\tvalid_1's rmse: 0.0834549\n",
      "[1775]\ttraining's rmse: 0.0807068\tvalid_1's rmse: 0.0834537\n",
      "[1800]\ttraining's rmse: 0.0807014\tvalid_1's rmse: 0.0834528\n",
      "[1825]\ttraining's rmse: 0.0806963\tvalid_1's rmse: 0.0834524\n",
      "[1850]\ttraining's rmse: 0.0806917\tvalid_1's rmse: 0.0834513\n",
      "[1875]\ttraining's rmse: 0.0806872\tvalid_1's rmse: 0.0834504\n",
      "[1900]\ttraining's rmse: 0.0806829\tvalid_1's rmse: 0.0834496\n",
      "[1925]\ttraining's rmse: 0.0806795\tvalid_1's rmse: 0.083449\n",
      "[1950]\ttraining's rmse: 0.080675\tvalid_1's rmse: 0.0834483\n",
      "[1975]\ttraining's rmse: 0.0806707\tvalid_1's rmse: 0.0834467\n",
      "[2000]\ttraining's rmse: 0.0806675\tvalid_1's rmse: 0.0834461\n",
      "[2025]\ttraining's rmse: 0.0806637\tvalid_1's rmse: 0.0834452\n",
      "[2050]\ttraining's rmse: 0.0806603\tvalid_1's rmse: 0.0834448\n",
      "[2075]\ttraining's rmse: 0.0806564\tvalid_1's rmse: 0.0834439\n",
      "[2100]\ttraining's rmse: 0.0806534\tvalid_1's rmse: 0.0834423\n",
      "[2125]\ttraining's rmse: 0.0806506\tvalid_1's rmse: 0.0834416\n",
      "[2150]\ttraining's rmse: 0.0806459\tvalid_1's rmse: 0.0834414\n",
      "[2175]\ttraining's rmse: 0.080642\tvalid_1's rmse: 0.0834411\n",
      "[2200]\ttraining's rmse: 0.0806393\tvalid_1's rmse: 0.0834409\n",
      "[2225]\ttraining's rmse: 0.0806363\tvalid_1's rmse: 0.0834399\n",
      "[2250]\ttraining's rmse: 0.0806335\tvalid_1's rmse: 0.0834392\n",
      "[2275]\ttraining's rmse: 0.0806316\tvalid_1's rmse: 0.0834393\n",
      "[2300]\ttraining's rmse: 0.0806287\tvalid_1's rmse: 0.083439\n",
      "[2325]\ttraining's rmse: 0.0806248\tvalid_1's rmse: 0.0834387\n",
      "[2350]\ttraining's rmse: 0.0806219\tvalid_1's rmse: 0.0834383\n",
      "[2375]\ttraining's rmse: 0.08062\tvalid_1's rmse: 0.083438\n",
      "[2400]\ttraining's rmse: 0.0806173\tvalid_1's rmse: 0.0834376\n",
      "[2425]\ttraining's rmse: 0.0806138\tvalid_1's rmse: 0.0834377\n",
      "[2450]\ttraining's rmse: 0.0806119\tvalid_1's rmse: 0.0834369\n",
      "[2475]\ttraining's rmse: 0.0806099\tvalid_1's rmse: 0.0834364\n",
      "[2500]\ttraining's rmse: 0.0806085\tvalid_1's rmse: 0.0834369\n",
      "[2525]\ttraining's rmse: 0.0806068\tvalid_1's rmse: 0.0834371\n",
      "Early stopping, best iteration is:\n",
      "[2480]\ttraining's rmse: 0.0806095\tvalid_1's rmse: 0.0834363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/home/ningzesun/.local/lib/python3.6/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[25]\ttraining's rmse: 0.0827814\tvalid_1's rmse: 0.0839151\n",
      "[50]\ttraining's rmse: 0.0826837\tvalid_1's rmse: 0.0838694\n",
      "[75]\ttraining's rmse: 0.0825849\tvalid_1's rmse: 0.083828\n",
      "[100]\ttraining's rmse: 0.0824969\tvalid_1's rmse: 0.08379\n",
      "[125]\ttraining's rmse: 0.0824069\tvalid_1's rmse: 0.083753\n",
      "[150]\ttraining's rmse: 0.0823192\tvalid_1's rmse: 0.0837186\n",
      "[175]\ttraining's rmse: 0.0822483\tvalid_1's rmse: 0.0836896\n",
      "[200]\ttraining's rmse: 0.0821731\tvalid_1's rmse: 0.0836606\n",
      "[225]\ttraining's rmse: 0.0821006\tvalid_1's rmse: 0.0836349\n",
      "[250]\ttraining's rmse: 0.0820363\tvalid_1's rmse: 0.083611\n",
      "[275]\ttraining's rmse: 0.0819767\tvalid_1's rmse: 0.0835884\n",
      "[300]\ttraining's rmse: 0.081916\tvalid_1's rmse: 0.0835665\n",
      "[325]\ttraining's rmse: 0.0818588\tvalid_1's rmse: 0.0835464\n",
      "[350]\ttraining's rmse: 0.081804\tvalid_1's rmse: 0.0835281\n",
      "[375]\ttraining's rmse: 0.0817578\tvalid_1's rmse: 0.0835125\n",
      "[400]\ttraining's rmse: 0.0817101\tvalid_1's rmse: 0.0834976\n",
      "[425]\ttraining's rmse: 0.0816681\tvalid_1's rmse: 0.0834856\n",
      "[450]\ttraining's rmse: 0.0816228\tvalid_1's rmse: 0.0834711\n",
      "[475]\ttraining's rmse: 0.0815851\tvalid_1's rmse: 0.0834607\n",
      "[500]\ttraining's rmse: 0.0815519\tvalid_1's rmse: 0.0834505\n",
      "[525]\ttraining's rmse: 0.0815107\tvalid_1's rmse: 0.0834394\n",
      "[550]\ttraining's rmse: 0.0814709\tvalid_1's rmse: 0.0834284\n",
      "[575]\ttraining's rmse: 0.0814359\tvalid_1's rmse: 0.0834188\n",
      "[600]\ttraining's rmse: 0.0814006\tvalid_1's rmse: 0.0834113\n",
      "[625]\ttraining's rmse: 0.0813734\tvalid_1's rmse: 0.0834047\n",
      "[650]\ttraining's rmse: 0.0813391\tvalid_1's rmse: 0.0833959\n",
      "[675]\ttraining's rmse: 0.0813047\tvalid_1's rmse: 0.0833881\n",
      "[700]\ttraining's rmse: 0.081274\tvalid_1's rmse: 0.0833805\n",
      "[725]\ttraining's rmse: 0.0812445\tvalid_1's rmse: 0.0833736\n",
      "[750]\ttraining's rmse: 0.0812164\tvalid_1's rmse: 0.0833672\n",
      "[775]\ttraining's rmse: 0.0811939\tvalid_1's rmse: 0.0833631\n",
      "[800]\ttraining's rmse: 0.0811675\tvalid_1's rmse: 0.0833569\n",
      "[825]\ttraining's rmse: 0.0811434\tvalid_1's rmse: 0.0833514\n",
      "[850]\ttraining's rmse: 0.0811203\tvalid_1's rmse: 0.0833471\n",
      "[875]\ttraining's rmse: 0.0810989\tvalid_1's rmse: 0.0833416\n",
      "[900]\ttraining's rmse: 0.081074\tvalid_1's rmse: 0.0833372\n",
      "[925]\ttraining's rmse: 0.08105\tvalid_1's rmse: 0.0833347\n",
      "[950]\ttraining's rmse: 0.0810322\tvalid_1's rmse: 0.0833304\n",
      "[975]\ttraining's rmse: 0.0810118\tvalid_1's rmse: 0.0833274\n",
      "[1000]\ttraining's rmse: 0.0809942\tvalid_1's rmse: 0.0833241\n",
      "[1025]\ttraining's rmse: 0.0809719\tvalid_1's rmse: 0.0833217\n",
      "[1050]\ttraining's rmse: 0.0809529\tvalid_1's rmse: 0.0833189\n",
      "[1075]\ttraining's rmse: 0.0809336\tvalid_1's rmse: 0.0833168\n",
      "[1100]\ttraining's rmse: 0.0809168\tvalid_1's rmse: 0.0833139\n",
      "[1125]\ttraining's rmse: 0.0808996\tvalid_1's rmse: 0.0833117\n",
      "[1150]\ttraining's rmse: 0.0808815\tvalid_1's rmse: 0.0833105\n",
      "[1175]\ttraining's rmse: 0.0808673\tvalid_1's rmse: 0.0833086\n",
      "[1200]\ttraining's rmse: 0.0808533\tvalid_1's rmse: 0.0833077\n",
      "[1225]\ttraining's rmse: 0.080837\tvalid_1's rmse: 0.0833051\n",
      "[1250]\ttraining's rmse: 0.0808231\tvalid_1's rmse: 0.083304\n",
      "[1275]\ttraining's rmse: 0.080808\tvalid_1's rmse: 0.083303\n",
      "[1300]\ttraining's rmse: 0.0807942\tvalid_1's rmse: 0.083302\n",
      "[1325]\ttraining's rmse: 0.0807804\tvalid_1's rmse: 0.0833005\n",
      "[1350]\ttraining's rmse: 0.080766\tvalid_1's rmse: 0.0832995\n",
      "[1375]\ttraining's rmse: 0.0807532\tvalid_1's rmse: 0.0832983\n",
      "[1400]\ttraining's rmse: 0.0807426\tvalid_1's rmse: 0.0832969\n",
      "[1425]\ttraining's rmse: 0.0807303\tvalid_1's rmse: 0.0832963\n",
      "[1450]\ttraining's rmse: 0.0807184\tvalid_1's rmse: 0.0832956\n",
      "[1475]\ttraining's rmse: 0.0807091\tvalid_1's rmse: 0.0832953\n",
      "[1500]\ttraining's rmse: 0.080698\tvalid_1's rmse: 0.0832946\n",
      "[1525]\ttraining's rmse: 0.080686\tvalid_1's rmse: 0.0832939\n",
      "[1550]\ttraining's rmse: 0.0806764\tvalid_1's rmse: 0.0832941\n",
      "[1575]\ttraining's rmse: 0.080668\tvalid_1's rmse: 0.083294\n",
      "[1600]\ttraining's rmse: 0.0806607\tvalid_1's rmse: 0.083294\n",
      "Early stopping, best iteration is:\n",
      "[1560]\ttraining's rmse: 0.0806722\tvalid_1's rmse: 0.0832937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/home/ningzesun/.local/lib/python3.6/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[25]\ttraining's rmse: 0.0838302\tvalid_1's rmse: 0.0796423\n",
      "[50]\ttraining's rmse: 0.0837351\tvalid_1's rmse: 0.0795881\n",
      "[75]\ttraining's rmse: 0.0836408\tvalid_1's rmse: 0.0795417\n",
      "[100]\ttraining's rmse: 0.0835574\tvalid_1's rmse: 0.0795029\n",
      "[125]\ttraining's rmse: 0.083472\tvalid_1's rmse: 0.0794645\n",
      "[150]\ttraining's rmse: 0.0833883\tvalid_1's rmse: 0.0794247\n",
      "[175]\ttraining's rmse: 0.0833191\tvalid_1's rmse: 0.0794004\n",
      "[200]\ttraining's rmse: 0.0832459\tvalid_1's rmse: 0.0793669\n",
      "[225]\ttraining's rmse: 0.0831763\tvalid_1's rmse: 0.0793344\n",
      "[250]\ttraining's rmse: 0.0831145\tvalid_1's rmse: 0.0793108\n",
      "[275]\ttraining's rmse: 0.0830588\tvalid_1's rmse: 0.079292\n",
      "[300]\ttraining's rmse: 0.083001\tvalid_1's rmse: 0.079267\n",
      "[325]\ttraining's rmse: 0.0829458\tvalid_1's rmse: 0.0792734\n",
      "[350]\ttraining's rmse: 0.0828927\tvalid_1's rmse: 0.0792494\n",
      "[375]\ttraining's rmse: 0.0828479\tvalid_1's rmse: 0.0792304\n",
      "[400]\ttraining's rmse: 0.0828005\tvalid_1's rmse: 0.0792117\n",
      "[425]\ttraining's rmse: 0.0827594\tvalid_1's rmse: 0.0791952\n",
      "[450]\ttraining's rmse: 0.0827149\tvalid_1's rmse: 0.0791778\n",
      "[475]\ttraining's rmse: 0.0826806\tvalid_1's rmse: 0.0791638\n",
      "[500]\ttraining's rmse: 0.0826504\tvalid_1's rmse: 0.0791553\n",
      "[525]\ttraining's rmse: 0.0826121\tvalid_1's rmse: 0.0791656\n",
      "[550]\ttraining's rmse: 0.0825743\tvalid_1's rmse: 0.0791512\n",
      "Early stopping, best iteration is:\n",
      "[520]\ttraining's rmse: 0.082618\tvalid_1's rmse: 0.0791423\n"
     ]
    }
   ],
   "source": [
    "score = 0\n",
    "kf = KFold(n_splits=5)\n",
    "models = []\n",
    "\n",
    "params = {\"objective\": \"regression\", \"boosting\": \"gbdt\", \"num_leaves\": 500, \n",
    "              \"learning_rate\": 0.002515, 'bagging_fraction': 0.8297, \"reg_lambda\": 0.1052, \n",
    "              'reg_alpha':0.1046, \"metric\": \"rmse\", 'max_depth': 10, 'min_child_weight': 27,\n",
    "              'verbose': -1, 'min_split_gain':0.08138 , 'subsample_freq':1, 'sub_feature':  0.4492}\n",
    "for train_index,test_index in kf.split(features):\n",
    "    train_features = features.loc[train_index]\n",
    "    train_target = target.loc[train_index]\n",
    "    test_features = features.loc[test_index]\n",
    "    test_target = target.loc[test_index]\n",
    "    d_training = lgb.Dataset(train_features, label=train_target, \n",
    "                             categorical_feature=categorical_features, free_raw_data=False)\n",
    "    d_test = lgb.Dataset(test_features, label=test_target, \n",
    "                         categorical_feature=categorical_features, free_raw_data=False)\n",
    "    model = lgb.train(params, train_set=d_training, num_boost_round=3000, \n",
    "                      valid_sets=[d_training,d_test], verbose_eval=25, early_stopping_rounds=50)\n",
    "    y_pred_valid = model.predict(test_features)\n",
    "    score += np.sqrt(mean_squared_error(test_target, y_pred_valid)) / 3\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1087.64 MB\n",
      "Memory usage after optimization is: 428.55 MB\n",
      "Decreased by 60.6%\n",
      "Memory usage of dataframe is 1227.28 MB\n",
      "Memory usage after optimization is: 483.58 MB\n",
      "Decreased by 60.6%\n"
     ]
    }
   ],
   "source": [
    "del df_drop\n",
    "gc.collect()\n",
    "df_sub_2016 = reduce_mem_usage(pd.read_csv('./output/final_sub_2016.csv')).drop_duplicates('parcelid')\n",
    "df_sub_2016['year'] = 0\n",
    "results = {}\n",
    "for month in [10,11,12]:\n",
    "    df_sub_2016['month'] = month\n",
    "    for i in models:\n",
    "        if  month not in results.keys():\n",
    "            results[month] = i.predict(df_sub_2016[list(features.columns)], \n",
    "                                       num_iteration=i.best_iteration) / len(models)\n",
    "        else:\n",
    "            results[month] += i.predict(df_sub_2016[list(features.columns)], \n",
    "                                        num_iteration=i.best_iteration) / len(models)\n",
    "for i in results.keys():\n",
    "    df_sub_2016[str(i)] = results[i]\n",
    "df_sub = pd.read_csv('./Resources/sample_submission.csv')\n",
    "df_sub = df_sub.rename(columns = {'ParcelId': 'parcelid'})\n",
    "df_sub = pd.merge(df_sub[['parcelid']], df_sub_2016[['parcelid', '10', '11', '12']].drop_duplicates('parcelid'),\n",
    "                  how = 'left', on = 'parcelid')\n",
    "df_sub = df_sub.rename(columns = {'10': '201610', '11': '201611', '12': '201612'}).drop_duplicates('parcelid')\n",
    "del df_sub_2016\n",
    "gc.collect()\n",
    "df_sub_2017 = reduce_mem_usage(pd.read_csv('./output/final_sub_2017.csv'))\n",
    "df_sub_2017['year'] = 1\n",
    "results = {}\n",
    "for month in [10,11,12]:\n",
    "    df_sub_2017['month'] = month\n",
    "    for i in models:\n",
    "        if  month not in results.keys():\n",
    "            results[month] = i.predict(df_sub_2017[list(features.columns)], \n",
    "                                       num_iteration=i.best_iteration) / len(models)\n",
    "        else:\n",
    "            results[month] += i.predict(df_sub_2017[list(features.columns)], \n",
    "                                        num_iteration=i.best_iteration) / len(models)\n",
    "for i in results.keys():\n",
    "    df_sub_2017[str(i)] = results[i]\n",
    "df_sub = pd.merge(df_sub[['parcelid', '201610', '201611', '201612']], \n",
    "                  df_sub_2017[['parcelid', '10', '11', '12']].drop_duplicates('parcelid'), \n",
    "                  how = 'left', on = 'parcelid')\n",
    "df_sub = df_sub.rename(columns = {'10': '201710', '11': '201711', '12': '201712'})\n",
    "del df_sub_2017\n",
    "gc.collect()\n",
    "df_sub.to_csv('./output/sample_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parcelid</th>\n",
       "      <th>201610</th>\n",
       "      <th>201611</th>\n",
       "      <th>201612</th>\n",
       "      <th>201710</th>\n",
       "      <th>201711</th>\n",
       "      <th>201712</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10754147</td>\n",
       "      <td>-0.015853</td>\n",
       "      <td>-0.015826</td>\n",
       "      <td>-0.015813</td>\n",
       "      <td>-0.011554</td>\n",
       "      <td>-0.011527</td>\n",
       "      <td>-0.011514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10759547</td>\n",
       "      <td>-0.011209</td>\n",
       "      <td>-0.011178</td>\n",
       "      <td>-0.011166</td>\n",
       "      <td>-0.006334</td>\n",
       "      <td>-0.006303</td>\n",
       "      <td>-0.006291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10843547</td>\n",
       "      <td>0.003648</td>\n",
       "      <td>0.003662</td>\n",
       "      <td>0.003662</td>\n",
       "      <td>0.005417</td>\n",
       "      <td>0.005451</td>\n",
       "      <td>0.005451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10859147</td>\n",
       "      <td>0.014715</td>\n",
       "      <td>0.014736</td>\n",
       "      <td>0.014749</td>\n",
       "      <td>0.014211</td>\n",
       "      <td>0.014232</td>\n",
       "      <td>0.014245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10879947</td>\n",
       "      <td>0.010276</td>\n",
       "      <td>0.010297</td>\n",
       "      <td>0.010310</td>\n",
       "      <td>0.010075</td>\n",
       "      <td>0.010096</td>\n",
       "      <td>0.010109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2985212</th>\n",
       "      <td>168176230</td>\n",
       "      <td>0.014003</td>\n",
       "      <td>0.014110</td>\n",
       "      <td>0.014110</td>\n",
       "      <td>0.017458</td>\n",
       "      <td>0.017477</td>\n",
       "      <td>0.017477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2985213</th>\n",
       "      <td>14273630</td>\n",
       "      <td>0.013737</td>\n",
       "      <td>0.013844</td>\n",
       "      <td>0.013844</td>\n",
       "      <td>0.018469</td>\n",
       "      <td>0.018473</td>\n",
       "      <td>0.018473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2985214</th>\n",
       "      <td>168040630</td>\n",
       "      <td>0.014003</td>\n",
       "      <td>0.014110</td>\n",
       "      <td>0.014110</td>\n",
       "      <td>0.010036</td>\n",
       "      <td>0.010045</td>\n",
       "      <td>0.010056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2985215</th>\n",
       "      <td>168040830</td>\n",
       "      <td>0.013829</td>\n",
       "      <td>0.013936</td>\n",
       "      <td>0.013936</td>\n",
       "      <td>0.054464</td>\n",
       "      <td>0.054530</td>\n",
       "      <td>0.054557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2985216</th>\n",
       "      <td>168040430</td>\n",
       "      <td>0.013829</td>\n",
       "      <td>0.013936</td>\n",
       "      <td>0.013936</td>\n",
       "      <td>0.010156</td>\n",
       "      <td>0.010159</td>\n",
       "      <td>0.010159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2985217 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          parcelid    201610    201611    201612    201710    201711    201712\n",
       "0         10754147 -0.015853 -0.015826 -0.015813 -0.011554 -0.011527 -0.011514\n",
       "1         10759547 -0.011209 -0.011178 -0.011166 -0.006334 -0.006303 -0.006291\n",
       "2         10843547  0.003648  0.003662  0.003662  0.005417  0.005451  0.005451\n",
       "3         10859147  0.014715  0.014736  0.014749  0.014211  0.014232  0.014245\n",
       "4         10879947  0.010276  0.010297  0.010310  0.010075  0.010096  0.010109\n",
       "...            ...       ...       ...       ...       ...       ...       ...\n",
       "2985212  168176230  0.014003  0.014110  0.014110  0.017458  0.017477  0.017477\n",
       "2985213   14273630  0.013737  0.013844  0.013844  0.018469  0.018473  0.018473\n",
       "2985214  168040630  0.014003  0.014110  0.014110  0.010036  0.010045  0.010056\n",
       "2985215  168040830  0.013829  0.013936  0.013936  0.054464  0.054530  0.054557\n",
       "2985216  168040430  0.013829  0.013936  0.013936  0.010156  0.010159  0.010159\n",
       "\n",
       "[2985217 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
