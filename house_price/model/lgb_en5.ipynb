{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "\n",
    "def reduce_mem_usage(df, use_float16=False):\n",
    "    \"\"\"\n",
    "    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    \n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n",
    "            continue\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
    "    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop = pd.read_csv('../output/outlier_remove.csv')\n",
    "categorical_features = ['airconditioningtypeid', 'hashottuborspa', 'heatingorsystemtypeid', \n",
    "                       'pooltypeid2', 'propertylandusetypeid', 'fips', 'regionidcounty', \n",
    "                       'buildingqualitytypeid_fill', 'regionidcity_fill', 'year', \n",
    "                       'regionidneighborhood_fill', 'taxdelinquencyflag']\n",
    "target = df_drop.logerror\n",
    "features = df_drop.drop(['logerror'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training (ensemble): 0 ...\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[25]\ttraining's rmse: 0.0828857\tvalid_1's rmse: 0.0835359\n",
      "[50]\ttraining's rmse: 0.0827942\tvalid_1's rmse: 0.0834708\n",
      "[75]\ttraining's rmse: 0.0827064\tvalid_1's rmse: 0.0834091\n",
      "[100]\ttraining's rmse: 0.082619\tvalid_1's rmse: 0.0833517\n",
      "[125]\ttraining's rmse: 0.0825447\tvalid_1's rmse: 0.0833013\n",
      "[150]\ttraining's rmse: 0.0824701\tvalid_1's rmse: 0.0832508\n",
      "[175]\ttraining's rmse: 0.0824003\tvalid_1's rmse: 0.0832047\n",
      "[200]\ttraining's rmse: 0.0823315\tvalid_1's rmse: 0.083162\n",
      "[225]\ttraining's rmse: 0.0822651\tvalid_1's rmse: 0.0831209\n",
      "[250]\ttraining's rmse: 0.0822065\tvalid_1's rmse: 0.0830839\n",
      "[275]\ttraining's rmse: 0.0821507\tvalid_1's rmse: 0.0830493\n",
      "[300]\ttraining's rmse: 0.0820955\tvalid_1's rmse: 0.0830166\n",
      "[325]\ttraining's rmse: 0.0820495\tvalid_1's rmse: 0.0829869\n",
      "[350]\ttraining's rmse: 0.0819969\tvalid_1's rmse: 0.0829573\n",
      "[375]\ttraining's rmse: 0.08195\tvalid_1's rmse: 0.0829288\n",
      "[400]\ttraining's rmse: 0.0819052\tvalid_1's rmse: 0.0829042\n",
      "[425]\ttraining's rmse: 0.0818612\tvalid_1's rmse: 0.0828803\n",
      "[450]\ttraining's rmse: 0.0818167\tvalid_1's rmse: 0.0828578\n",
      "[475]\ttraining's rmse: 0.0817797\tvalid_1's rmse: 0.0828372\n",
      "[500]\ttraining's rmse: 0.0817398\tvalid_1's rmse: 0.082816\n",
      "[525]\ttraining's rmse: 0.081701\tvalid_1's rmse: 0.0827961\n",
      "[550]\ttraining's rmse: 0.0816683\tvalid_1's rmse: 0.0827778\n",
      "[575]\ttraining's rmse: 0.0816326\tvalid_1's rmse: 0.0827597\n",
      "[600]\ttraining's rmse: 0.0816002\tvalid_1's rmse: 0.0827444\n",
      "[625]\ttraining's rmse: 0.0815698\tvalid_1's rmse: 0.0827293\n",
      "[650]\ttraining's rmse: 0.0815404\tvalid_1's rmse: 0.0827155\n",
      "[675]\ttraining's rmse: 0.0815149\tvalid_1's rmse: 0.0827016\n",
      "[700]\ttraining's rmse: 0.0814878\tvalid_1's rmse: 0.0826889\n",
      "[725]\ttraining's rmse: 0.0814597\tvalid_1's rmse: 0.0826769\n",
      "[750]\ttraining's rmse: 0.0814344\tvalid_1's rmse: 0.0826656\n",
      "[775]\ttraining's rmse: 0.08141\tvalid_1's rmse: 0.0826547\n",
      "[800]\ttraining's rmse: 0.0813836\tvalid_1's rmse: 0.0826446\n",
      "[825]\ttraining's rmse: 0.0813635\tvalid_1's rmse: 0.0826345\n",
      "[850]\ttraining's rmse: 0.0813417\tvalid_1's rmse: 0.0826254\n",
      "[875]\ttraining's rmse: 0.0813192\tvalid_1's rmse: 0.0826163\n",
      "[900]\ttraining's rmse: 0.0812978\tvalid_1's rmse: 0.0826072\n",
      "[925]\ttraining's rmse: 0.0812807\tvalid_1's rmse: 0.0825995\n",
      "[950]\ttraining's rmse: 0.0812575\tvalid_1's rmse: 0.0825909\n",
      "[975]\ttraining's rmse: 0.0812358\tvalid_1's rmse: 0.0825836\n",
      "[1000]\ttraining's rmse: 0.0812174\tvalid_1's rmse: 0.0825772\n",
      "[1025]\ttraining's rmse: 0.0811989\tvalid_1's rmse: 0.0825704\n",
      "[1050]\ttraining's rmse: 0.0811793\tvalid_1's rmse: 0.0825639\n",
      "[1075]\ttraining's rmse: 0.0811645\tvalid_1's rmse: 0.0825578\n",
      "[1100]\ttraining's rmse: 0.0811474\tvalid_1's rmse: 0.0825518\n",
      "[1125]\ttraining's rmse: 0.0811327\tvalid_1's rmse: 0.0825462\n",
      "[1150]\ttraining's rmse: 0.0811186\tvalid_1's rmse: 0.0825415\n",
      "[1175]\ttraining's rmse: 0.0811021\tvalid_1's rmse: 0.0825368\n",
      "[1200]\ttraining's rmse: 0.0810899\tvalid_1's rmse: 0.0825318\n",
      "[1225]\ttraining's rmse: 0.0810754\tvalid_1's rmse: 0.0825272\n",
      "[1250]\ttraining's rmse: 0.0810634\tvalid_1's rmse: 0.0825229\n",
      "[1275]\ttraining's rmse: 0.0810517\tvalid_1's rmse: 0.0825191\n",
      "[1300]\ttraining's rmse: 0.081038\tvalid_1's rmse: 0.0825149\n",
      "[1325]\ttraining's rmse: 0.0810262\tvalid_1's rmse: 0.0825115\n",
      "[1350]\ttraining's rmse: 0.0810135\tvalid_1's rmse: 0.0825078\n",
      "[1375]\ttraining's rmse: 0.0810049\tvalid_1's rmse: 0.0825045\n",
      "[1400]\ttraining's rmse: 0.0809973\tvalid_1's rmse: 0.0825013\n",
      "[1425]\ttraining's rmse: 0.0809856\tvalid_1's rmse: 0.0824981\n",
      "[1450]\ttraining's rmse: 0.0809789\tvalid_1's rmse: 0.0824951\n",
      "[1475]\ttraining's rmse: 0.0809712\tvalid_1's rmse: 0.0824922\n",
      "[1500]\ttraining's rmse: 0.0809636\tvalid_1's rmse: 0.0824899\n",
      "[1525]\ttraining's rmse: 0.0809535\tvalid_1's rmse: 0.0824877\n",
      "[1550]\ttraining's rmse: 0.0809463\tvalid_1's rmse: 0.0824855\n",
      "[1575]\ttraining's rmse: 0.0809384\tvalid_1's rmse: 0.0824836\n",
      "[1600]\ttraining's rmse: 0.0809315\tvalid_1's rmse: 0.0824808\n",
      "[1625]\ttraining's rmse: 0.0809264\tvalid_1's rmse: 0.0824789\n",
      "[1650]\ttraining's rmse: 0.0809199\tvalid_1's rmse: 0.0824776\n",
      "[1675]\ttraining's rmse: 0.0809134\tvalid_1's rmse: 0.0824764\n",
      "[1700]\ttraining's rmse: 0.0809075\tvalid_1's rmse: 0.082475\n",
      "[1725]\ttraining's rmse: 0.0809015\tvalid_1's rmse: 0.0824736\n",
      "[1750]\ttraining's rmse: 0.080896\tvalid_1's rmse: 0.0824726\n",
      "[1775]\ttraining's rmse: 0.0808903\tvalid_1's rmse: 0.0824711\n",
      "[1800]\ttraining's rmse: 0.0808849\tvalid_1's rmse: 0.0824698\n",
      "[1825]\ttraining's rmse: 0.0808796\tvalid_1's rmse: 0.0824684\n",
      "[1850]\ttraining's rmse: 0.0808732\tvalid_1's rmse: 0.0824676\n",
      "[1875]\ttraining's rmse: 0.0808701\tvalid_1's rmse: 0.0824667\n",
      "[1900]\ttraining's rmse: 0.0808654\tvalid_1's rmse: 0.0824664\n",
      "[1925]\ttraining's rmse: 0.0808629\tvalid_1's rmse: 0.0824659\n",
      "[1950]\ttraining's rmse: 0.0808589\tvalid_1's rmse: 0.0824647\n",
      "[1975]\ttraining's rmse: 0.0808542\tvalid_1's rmse: 0.0824636\n",
      "[2000]\ttraining's rmse: 0.0808495\tvalid_1's rmse: 0.0824627\n",
      "[2025]\ttraining's rmse: 0.0808459\tvalid_1's rmse: 0.082462\n",
      "[2050]\ttraining's rmse: 0.0808414\tvalid_1's rmse: 0.0824615\n",
      "[2075]\ttraining's rmse: 0.080838\tvalid_1's rmse: 0.082461\n",
      "[2100]\ttraining's rmse: 0.0808345\tvalid_1's rmse: 0.0824599\n",
      "[2125]\ttraining's rmse: 0.0808321\tvalid_1's rmse: 0.0824593\n",
      "[2150]\ttraining's rmse: 0.0808297\tvalid_1's rmse: 0.0824585\n",
      "[2175]\ttraining's rmse: 0.0808274\tvalid_1's rmse: 0.0824579\n",
      "[2200]\ttraining's rmse: 0.0808239\tvalid_1's rmse: 0.082457\n",
      "[2225]\ttraining's rmse: 0.0808209\tvalid_1's rmse: 0.0824563\n",
      "[2250]\ttraining's rmse: 0.0808188\tvalid_1's rmse: 0.082456\n",
      "[2275]\ttraining's rmse: 0.0808173\tvalid_1's rmse: 0.0824558\n",
      "[2300]\ttraining's rmse: 0.0808143\tvalid_1's rmse: 0.0824551\n",
      "[2325]\ttraining's rmse: 0.0808119\tvalid_1's rmse: 0.0824548\n",
      "[2350]\ttraining's rmse: 0.0808098\tvalid_1's rmse: 0.0824543\n",
      "[2375]\ttraining's rmse: 0.0808075\tvalid_1's rmse: 0.0824536\n",
      "[2400]\ttraining's rmse: 0.0808049\tvalid_1's rmse: 0.0824532\n",
      "[2425]\ttraining's rmse: 0.0808029\tvalid_1's rmse: 0.0824527\n",
      "[2450]\ttraining's rmse: 0.0808001\tvalid_1's rmse: 0.0824524\n",
      "[2475]\ttraining's rmse: 0.0807983\tvalid_1's rmse: 0.082452\n",
      "[2500]\ttraining's rmse: 0.0807963\tvalid_1's rmse: 0.0824515\n",
      "[2525]\ttraining's rmse: 0.0807951\tvalid_1's rmse: 0.0824512\n",
      "[2550]\ttraining's rmse: 0.0807933\tvalid_1's rmse: 0.0824507\n",
      "[2575]\ttraining's rmse: 0.0807906\tvalid_1's rmse: 0.0824501\n",
      "[2600]\ttraining's rmse: 0.0807878\tvalid_1's rmse: 0.08245\n",
      "[2625]\ttraining's rmse: 0.0807854\tvalid_1's rmse: 0.0824498\n",
      "[2650]\ttraining's rmse: 0.0807839\tvalid_1's rmse: 0.0824494\n",
      "[2675]\ttraining's rmse: 0.0807824\tvalid_1's rmse: 0.0824494\n",
      "[2700]\ttraining's rmse: 0.0807794\tvalid_1's rmse: 0.082449\n",
      "[2725]\ttraining's rmse: 0.0807774\tvalid_1's rmse: 0.0824485\n",
      "[2750]\ttraining's rmse: 0.0807768\tvalid_1's rmse: 0.0824485\n",
      "[2775]\ttraining's rmse: 0.0807759\tvalid_1's rmse: 0.0824483\n",
      "[2800]\ttraining's rmse: 0.0807746\tvalid_1's rmse: 0.0824483\n",
      "[2825]\ttraining's rmse: 0.0807729\tvalid_1's rmse: 0.0824481\n",
      "[2850]\ttraining's rmse: 0.0807712\tvalid_1's rmse: 0.082448\n",
      "[2875]\ttraining's rmse: 0.0807689\tvalid_1's rmse: 0.0824481\n",
      "Early stopping, best iteration is:\n",
      "[2832]\ttraining's rmse: 0.0807728\tvalid_1's rmse: 0.0824479\n",
      "\n",
      "Training (ensemble): 1 ...\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[25]\ttraining's rmse: 0.0828847\tvalid_1's rmse: 0.0835349\n",
      "[50]\ttraining's rmse: 0.0827911\tvalid_1's rmse: 0.0834691\n",
      "[75]\ttraining's rmse: 0.0826989\tvalid_1's rmse: 0.0834063\n",
      "[100]\ttraining's rmse: 0.0826163\tvalid_1's rmse: 0.0833502\n",
      "[125]\ttraining's rmse: 0.0825375\tvalid_1's rmse: 0.0832982\n",
      "[150]\ttraining's rmse: 0.082467\tvalid_1's rmse: 0.0832507\n",
      "[175]\ttraining's rmse: 0.0823958\tvalid_1's rmse: 0.083205\n",
      "[200]\ttraining's rmse: 0.0823248\tvalid_1's rmse: 0.0831584\n",
      "[225]\ttraining's rmse: 0.0822604\tvalid_1's rmse: 0.0831191\n",
      "[250]\ttraining's rmse: 0.0822049\tvalid_1's rmse: 0.083083\n",
      "[275]\ttraining's rmse: 0.0821483\tvalid_1's rmse: 0.0830467\n",
      "[300]\ttraining's rmse: 0.0820914\tvalid_1's rmse: 0.0830122\n",
      "[325]\ttraining's rmse: 0.0820393\tvalid_1's rmse: 0.0829797\n",
      "[350]\ttraining's rmse: 0.0819914\tvalid_1's rmse: 0.0829524\n",
      "[375]\ttraining's rmse: 0.081946\tvalid_1's rmse: 0.0829277\n",
      "[400]\ttraining's rmse: 0.0819025\tvalid_1's rmse: 0.0829028\n",
      "[425]\ttraining's rmse: 0.0818587\tvalid_1's rmse: 0.0828782\n",
      "[450]\ttraining's rmse: 0.0818194\tvalid_1's rmse: 0.0828566\n",
      "[475]\ttraining's rmse: 0.0817784\tvalid_1's rmse: 0.0828358\n",
      "[500]\ttraining's rmse: 0.0817426\tvalid_1's rmse: 0.0828161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[525]\ttraining's rmse: 0.081706\tvalid_1's rmse: 0.0827966\n",
      "[550]\ttraining's rmse: 0.0816718\tvalid_1's rmse: 0.082779\n",
      "[575]\ttraining's rmse: 0.0816408\tvalid_1's rmse: 0.0827618\n",
      "[600]\ttraining's rmse: 0.0816112\tvalid_1's rmse: 0.0827466\n",
      "[625]\ttraining's rmse: 0.081583\tvalid_1's rmse: 0.082732\n",
      "[650]\ttraining's rmse: 0.081555\tvalid_1's rmse: 0.0827183\n",
      "[675]\ttraining's rmse: 0.0815279\tvalid_1's rmse: 0.0827056\n",
      "[700]\ttraining's rmse: 0.0814999\tvalid_1's rmse: 0.0826933\n",
      "[725]\ttraining's rmse: 0.0814697\tvalid_1's rmse: 0.0826802\n",
      "[750]\ttraining's rmse: 0.081445\tvalid_1's rmse: 0.0826686\n",
      "[775]\ttraining's rmse: 0.0814199\tvalid_1's rmse: 0.0826576\n",
      "[800]\ttraining's rmse: 0.0813976\tvalid_1's rmse: 0.0826467\n",
      "[825]\ttraining's rmse: 0.0813714\tvalid_1's rmse: 0.0826367\n",
      "[850]\ttraining's rmse: 0.0813469\tvalid_1's rmse: 0.0826269\n",
      "[875]\ttraining's rmse: 0.0813286\tvalid_1's rmse: 0.0826181\n",
      "[900]\ttraining's rmse: 0.0813097\tvalid_1's rmse: 0.0826103\n",
      "[925]\ttraining's rmse: 0.0812893\tvalid_1's rmse: 0.0826013\n",
      "[950]\ttraining's rmse: 0.0812699\tvalid_1's rmse: 0.0825932\n",
      "[975]\ttraining's rmse: 0.0812506\tvalid_1's rmse: 0.0825857\n",
      "[1000]\ttraining's rmse: 0.0812338\tvalid_1's rmse: 0.0825788\n",
      "[1025]\ttraining's rmse: 0.0812142\tvalid_1's rmse: 0.0825724\n",
      "[1050]\ttraining's rmse: 0.0811974\tvalid_1's rmse: 0.0825661\n",
      "[1075]\ttraining's rmse: 0.0811826\tvalid_1's rmse: 0.0825602\n",
      "[1100]\ttraining's rmse: 0.0811642\tvalid_1's rmse: 0.0825543\n",
      "[1125]\ttraining's rmse: 0.0811472\tvalid_1's rmse: 0.0825491\n",
      "[1150]\ttraining's rmse: 0.0811316\tvalid_1's rmse: 0.082545\n",
      "[1175]\ttraining's rmse: 0.0811174\tvalid_1's rmse: 0.0825396\n",
      "[1200]\ttraining's rmse: 0.0811047\tvalid_1's rmse: 0.082536\n",
      "[1225]\ttraining's rmse: 0.0810917\tvalid_1's rmse: 0.0825315\n",
      "[1250]\ttraining's rmse: 0.0810809\tvalid_1's rmse: 0.0825276\n",
      "[1275]\ttraining's rmse: 0.081069\tvalid_1's rmse: 0.0825236\n",
      "[1300]\ttraining's rmse: 0.0810551\tvalid_1's rmse: 0.0825205\n",
      "[1325]\ttraining's rmse: 0.0810447\tvalid_1's rmse: 0.0825169\n",
      "[1350]\ttraining's rmse: 0.0810333\tvalid_1's rmse: 0.0825128\n",
      "[1375]\ttraining's rmse: 0.0810206\tvalid_1's rmse: 0.0825089\n",
      "[1400]\ttraining's rmse: 0.0810085\tvalid_1's rmse: 0.082506\n",
      "[1425]\ttraining's rmse: 0.0809967\tvalid_1's rmse: 0.0825022\n",
      "[1450]\ttraining's rmse: 0.0809901\tvalid_1's rmse: 0.0825002\n",
      "[1475]\ttraining's rmse: 0.0809805\tvalid_1's rmse: 0.0824974\n",
      "[1500]\ttraining's rmse: 0.0809714\tvalid_1's rmse: 0.0824948\n",
      "[1525]\ttraining's rmse: 0.0809628\tvalid_1's rmse: 0.0824924\n",
      "[1550]\ttraining's rmse: 0.0809557\tvalid_1's rmse: 0.08249\n",
      "[1575]\ttraining's rmse: 0.080951\tvalid_1's rmse: 0.082488\n",
      "[1600]\ttraining's rmse: 0.0809432\tvalid_1's rmse: 0.0824861\n",
      "[1625]\ttraining's rmse: 0.0809361\tvalid_1's rmse: 0.0824843\n",
      "[1650]\ttraining's rmse: 0.0809295\tvalid_1's rmse: 0.0824815\n",
      "[1675]\ttraining's rmse: 0.0809249\tvalid_1's rmse: 0.0824797\n",
      "[1700]\ttraining's rmse: 0.0809199\tvalid_1's rmse: 0.0824784\n",
      "[1725]\ttraining's rmse: 0.0809155\tvalid_1's rmse: 0.0824769\n",
      "[1750]\ttraining's rmse: 0.0809111\tvalid_1's rmse: 0.082475\n",
      "[1775]\ttraining's rmse: 0.0809061\tvalid_1's rmse: 0.0824731\n",
      "[1800]\ttraining's rmse: 0.0809006\tvalid_1's rmse: 0.0824716\n",
      "[1825]\ttraining's rmse: 0.0808955\tvalid_1's rmse: 0.0824705\n",
      "[1850]\ttraining's rmse: 0.0808903\tvalid_1's rmse: 0.0824695\n",
      "[1875]\ttraining's rmse: 0.0808865\tvalid_1's rmse: 0.0824687\n",
      "[1900]\ttraining's rmse: 0.0808822\tvalid_1's rmse: 0.0824679\n",
      "[1925]\ttraining's rmse: 0.0808757\tvalid_1's rmse: 0.0824669\n",
      "[1950]\ttraining's rmse: 0.0808711\tvalid_1's rmse: 0.0824657\n",
      "[1975]\ttraining's rmse: 0.0808661\tvalid_1's rmse: 0.0824645\n",
      "[2000]\ttraining's rmse: 0.0808631\tvalid_1's rmse: 0.0824637\n",
      "[2025]\ttraining's rmse: 0.0808589\tvalid_1's rmse: 0.0824626\n",
      "[2050]\ttraining's rmse: 0.080856\tvalid_1's rmse: 0.0824617\n",
      "[2075]\ttraining's rmse: 0.0808529\tvalid_1's rmse: 0.0824607\n",
      "[2100]\ttraining's rmse: 0.0808505\tvalid_1's rmse: 0.0824596\n",
      "[2125]\ttraining's rmse: 0.0808471\tvalid_1's rmse: 0.0824584\n",
      "[2150]\ttraining's rmse: 0.0808442\tvalid_1's rmse: 0.0824579\n",
      "[2175]\ttraining's rmse: 0.0808422\tvalid_1's rmse: 0.0824575\n",
      "[2200]\ttraining's rmse: 0.0808388\tvalid_1's rmse: 0.0824568\n",
      "[2225]\ttraining's rmse: 0.0808345\tvalid_1's rmse: 0.0824561\n",
      "[2250]\ttraining's rmse: 0.0808325\tvalid_1's rmse: 0.0824557\n",
      "[2275]\ttraining's rmse: 0.0808304\tvalid_1's rmse: 0.0824552\n",
      "[2300]\ttraining's rmse: 0.0808275\tvalid_1's rmse: 0.0824543\n",
      "[2325]\ttraining's rmse: 0.0808242\tvalid_1's rmse: 0.0824534\n",
      "[2350]\ttraining's rmse: 0.0808216\tvalid_1's rmse: 0.0824531\n",
      "[2375]\ttraining's rmse: 0.0808184\tvalid_1's rmse: 0.0824524\n",
      "[2400]\ttraining's rmse: 0.0808147\tvalid_1's rmse: 0.0824521\n",
      "[2425]\ttraining's rmse: 0.0808125\tvalid_1's rmse: 0.0824517\n",
      "[2450]\ttraining's rmse: 0.0808098\tvalid_1's rmse: 0.0824511\n",
      "[2475]\ttraining's rmse: 0.0808066\tvalid_1's rmse: 0.0824503\n",
      "[2500]\ttraining's rmse: 0.0808049\tvalid_1's rmse: 0.0824501\n",
      "[2525]\ttraining's rmse: 0.0808028\tvalid_1's rmse: 0.0824496\n",
      "[2550]\ttraining's rmse: 0.0808007\tvalid_1's rmse: 0.0824491\n",
      "[2575]\ttraining's rmse: 0.0807994\tvalid_1's rmse: 0.0824489\n",
      "[2600]\ttraining's rmse: 0.0807973\tvalid_1's rmse: 0.0824484\n",
      "[2625]\ttraining's rmse: 0.0807957\tvalid_1's rmse: 0.0824479\n",
      "[2650]\ttraining's rmse: 0.0807931\tvalid_1's rmse: 0.0824467\n",
      "[2675]\ttraining's rmse: 0.0807919\tvalid_1's rmse: 0.0824467\n",
      "[2700]\ttraining's rmse: 0.08079\tvalid_1's rmse: 0.0824464\n",
      "[2725]\ttraining's rmse: 0.0807877\tvalid_1's rmse: 0.082446\n",
      "[2750]\ttraining's rmse: 0.080786\tvalid_1's rmse: 0.0824459\n",
      "[2775]\ttraining's rmse: 0.0807841\tvalid_1's rmse: 0.0824455\n",
      "[2800]\ttraining's rmse: 0.0807831\tvalid_1's rmse: 0.0824453\n",
      "[2825]\ttraining's rmse: 0.0807819\tvalid_1's rmse: 0.082445\n",
      "[2850]\ttraining's rmse: 0.0807804\tvalid_1's rmse: 0.0824448\n",
      "[2875]\ttraining's rmse: 0.0807788\tvalid_1's rmse: 0.0824447\n",
      "[2900]\ttraining's rmse: 0.0807773\tvalid_1's rmse: 0.0824445\n",
      "[2925]\ttraining's rmse: 0.0807761\tvalid_1's rmse: 0.0824442\n",
      "[2950]\ttraining's rmse: 0.0807748\tvalid_1's rmse: 0.0824439\n",
      "[2975]\ttraining's rmse: 0.0807734\tvalid_1's rmse: 0.0824437\n",
      "[3000]\ttraining's rmse: 0.0807713\tvalid_1's rmse: 0.0824436\n",
      "[3025]\ttraining's rmse: 0.0807691\tvalid_1's rmse: 0.0824432\n",
      "[3050]\ttraining's rmse: 0.0807682\tvalid_1's rmse: 0.082443\n",
      "[3075]\ttraining's rmse: 0.0807669\tvalid_1's rmse: 0.0824427\n",
      "[3100]\ttraining's rmse: 0.0807661\tvalid_1's rmse: 0.0824426\n",
      "[3125]\ttraining's rmse: 0.0807651\tvalid_1's rmse: 0.0824425\n",
      "[3150]\ttraining's rmse: 0.0807635\tvalid_1's rmse: 0.0824424\n",
      "[3175]\ttraining's rmse: 0.080762\tvalid_1's rmse: 0.0824422\n",
      "[3200]\ttraining's rmse: 0.0807601\tvalid_1's rmse: 0.0824422\n",
      "[3225]\ttraining's rmse: 0.0807596\tvalid_1's rmse: 0.0824421\n",
      "[3250]\ttraining's rmse: 0.0807588\tvalid_1's rmse: 0.082442\n",
      "[3275]\ttraining's rmse: 0.0807578\tvalid_1's rmse: 0.082442\n",
      "[3300]\ttraining's rmse: 0.0807573\tvalid_1's rmse: 0.0824421\n",
      "[3325]\ttraining's rmse: 0.0807568\tvalid_1's rmse: 0.0824422\n",
      "Early stopping, best iteration is:\n",
      "[3286]\ttraining's rmse: 0.0807576\tvalid_1's rmse: 0.0824419\n",
      "\n",
      "Training (ensemble): 2 ...\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[25]\ttraining's rmse: 0.0828912\tvalid_1's rmse: 0.0835371\n",
      "[50]\ttraining's rmse: 0.0827979\tvalid_1's rmse: 0.0834723\n",
      "[75]\ttraining's rmse: 0.0827045\tvalid_1's rmse: 0.0834075\n",
      "[100]\ttraining's rmse: 0.0826242\tvalid_1's rmse: 0.0833514\n",
      "[125]\ttraining's rmse: 0.0825412\tvalid_1's rmse: 0.0832967\n",
      "[150]\ttraining's rmse: 0.0824637\tvalid_1's rmse: 0.0832458\n",
      "[175]\ttraining's rmse: 0.0823911\tvalid_1's rmse: 0.083198\n",
      "[200]\ttraining's rmse: 0.0823255\tvalid_1's rmse: 0.0831561\n",
      "[225]\ttraining's rmse: 0.0822564\tvalid_1's rmse: 0.0831143\n",
      "[250]\ttraining's rmse: 0.082192\tvalid_1's rmse: 0.0830755\n",
      "[275]\ttraining's rmse: 0.0821345\tvalid_1's rmse: 0.08304\n",
      "[300]\ttraining's rmse: 0.0820872\tvalid_1's rmse: 0.0830097\n",
      "[325]\ttraining's rmse: 0.0820435\tvalid_1's rmse: 0.08298\n",
      "[350]\ttraining's rmse: 0.0820003\tvalid_1's rmse: 0.0829524\n",
      "[375]\ttraining's rmse: 0.0819532\tvalid_1's rmse: 0.0829262\n",
      "[400]\ttraining's rmse: 0.0819099\tvalid_1's rmse: 0.0829005\n",
      "[425]\ttraining's rmse: 0.0818634\tvalid_1's rmse: 0.0828763\n",
      "[450]\ttraining's rmse: 0.081821\tvalid_1's rmse: 0.0828527\n",
      "[475]\ttraining's rmse: 0.0817814\tvalid_1's rmse: 0.0828318\n",
      "[500]\ttraining's rmse: 0.0817457\tvalid_1's rmse: 0.0828125\n",
      "[525]\ttraining's rmse: 0.0817088\tvalid_1's rmse: 0.0827947\n",
      "[550]\ttraining's rmse: 0.0816784\tvalid_1's rmse: 0.0827781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[575]\ttraining's rmse: 0.081644\tvalid_1's rmse: 0.0827616\n",
      "[600]\ttraining's rmse: 0.0816121\tvalid_1's rmse: 0.082747\n",
      "[625]\ttraining's rmse: 0.0815799\tvalid_1's rmse: 0.0827316\n",
      "[650]\ttraining's rmse: 0.0815536\tvalid_1's rmse: 0.0827183\n",
      "[675]\ttraining's rmse: 0.0815199\tvalid_1's rmse: 0.0827046\n",
      "[700]\ttraining's rmse: 0.0814959\tvalid_1's rmse: 0.0826926\n",
      "[725]\ttraining's rmse: 0.0814684\tvalid_1's rmse: 0.0826807\n",
      "[750]\ttraining's rmse: 0.0814466\tvalid_1's rmse: 0.0826716\n",
      "[775]\ttraining's rmse: 0.0814216\tvalid_1's rmse: 0.0826607\n",
      "[800]\ttraining's rmse: 0.0813965\tvalid_1's rmse: 0.0826497\n",
      "[825]\ttraining's rmse: 0.0813758\tvalid_1's rmse: 0.0826387\n",
      "[850]\ttraining's rmse: 0.0813536\tvalid_1's rmse: 0.0826307\n",
      "[875]\ttraining's rmse: 0.0813367\tvalid_1's rmse: 0.0826218\n",
      "[900]\ttraining's rmse: 0.081314\tvalid_1's rmse: 0.0826126\n",
      "[925]\ttraining's rmse: 0.0812925\tvalid_1's rmse: 0.082603\n",
      "[950]\ttraining's rmse: 0.0812697\tvalid_1's rmse: 0.0825947\n",
      "[975]\ttraining's rmse: 0.0812537\tvalid_1's rmse: 0.0825875\n",
      "[1000]\ttraining's rmse: 0.0812355\tvalid_1's rmse: 0.0825802\n",
      "[1025]\ttraining's rmse: 0.0812175\tvalid_1's rmse: 0.0825742\n",
      "[1050]\ttraining's rmse: 0.0812004\tvalid_1's rmse: 0.0825674\n",
      "[1075]\ttraining's rmse: 0.0811844\tvalid_1's rmse: 0.0825612\n",
      "[1100]\ttraining's rmse: 0.0811643\tvalid_1's rmse: 0.0825547\n",
      "[1125]\ttraining's rmse: 0.081149\tvalid_1's rmse: 0.0825485\n",
      "[1150]\ttraining's rmse: 0.0811333\tvalid_1's rmse: 0.0825429\n",
      "[1175]\ttraining's rmse: 0.0811143\tvalid_1's rmse: 0.0825375\n",
      "[1200]\ttraining's rmse: 0.0811024\tvalid_1's rmse: 0.0825337\n",
      "[1225]\ttraining's rmse: 0.081085\tvalid_1's rmse: 0.0825287\n",
      "[1250]\ttraining's rmse: 0.0810716\tvalid_1's rmse: 0.082524\n",
      "[1275]\ttraining's rmse: 0.0810558\tvalid_1's rmse: 0.0825205\n",
      "[1300]\ttraining's rmse: 0.0810432\tvalid_1's rmse: 0.0825163\n",
      "[1325]\ttraining's rmse: 0.0810304\tvalid_1's rmse: 0.0825121\n",
      "[1350]\ttraining's rmse: 0.0810199\tvalid_1's rmse: 0.0825086\n",
      "[1375]\ttraining's rmse: 0.0810119\tvalid_1's rmse: 0.0825053\n",
      "[1400]\ttraining's rmse: 0.0810015\tvalid_1's rmse: 0.0825021\n",
      "[1425]\ttraining's rmse: 0.0809903\tvalid_1's rmse: 0.0824981\n",
      "[1450]\ttraining's rmse: 0.0809813\tvalid_1's rmse: 0.0824948\n",
      "[1475]\ttraining's rmse: 0.0809711\tvalid_1's rmse: 0.0824921\n",
      "[1500]\ttraining's rmse: 0.0809619\tvalid_1's rmse: 0.0824899\n",
      "[1525]\ttraining's rmse: 0.0809548\tvalid_1's rmse: 0.0824881\n",
      "[1550]\ttraining's rmse: 0.0809473\tvalid_1's rmse: 0.0824856\n",
      "[1575]\ttraining's rmse: 0.0809406\tvalid_1's rmse: 0.0824833\n",
      "[1600]\ttraining's rmse: 0.080931\tvalid_1's rmse: 0.0824807\n",
      "[1625]\ttraining's rmse: 0.0809246\tvalid_1's rmse: 0.0824787\n",
      "[1650]\ttraining's rmse: 0.0809169\tvalid_1's rmse: 0.0824767\n",
      "[1675]\ttraining's rmse: 0.0809103\tvalid_1's rmse: 0.082475\n",
      "[1700]\ttraining's rmse: 0.0809056\tvalid_1's rmse: 0.0824737\n",
      "[1725]\ttraining's rmse: 0.0809\tvalid_1's rmse: 0.0824719\n",
      "[1750]\ttraining's rmse: 0.0808944\tvalid_1's rmse: 0.0824707\n",
      "[1775]\ttraining's rmse: 0.0808888\tvalid_1's rmse: 0.0824691\n",
      "[1800]\ttraining's rmse: 0.0808838\tvalid_1's rmse: 0.0824673\n",
      "[1825]\ttraining's rmse: 0.080879\tvalid_1's rmse: 0.0824661\n",
      "[1850]\ttraining's rmse: 0.0808743\tvalid_1's rmse: 0.0824649\n",
      "[1875]\ttraining's rmse: 0.0808708\tvalid_1's rmse: 0.0824642\n",
      "[1900]\ttraining's rmse: 0.0808667\tvalid_1's rmse: 0.0824632\n",
      "[1925]\ttraining's rmse: 0.0808641\tvalid_1's rmse: 0.0824624\n",
      "[1950]\ttraining's rmse: 0.0808597\tvalid_1's rmse: 0.0824614\n",
      "[1975]\ttraining's rmse: 0.0808562\tvalid_1's rmse: 0.0824602\n",
      "[2000]\ttraining's rmse: 0.0808528\tvalid_1's rmse: 0.0824594\n",
      "[2025]\ttraining's rmse: 0.0808495\tvalid_1's rmse: 0.0824591\n",
      "[2050]\ttraining's rmse: 0.0808455\tvalid_1's rmse: 0.0824588\n",
      "[2075]\ttraining's rmse: 0.0808428\tvalid_1's rmse: 0.0824579\n",
      "[2100]\ttraining's rmse: 0.0808393\tvalid_1's rmse: 0.082457\n",
      "[2125]\ttraining's rmse: 0.0808347\tvalid_1's rmse: 0.0824561\n",
      "[2150]\ttraining's rmse: 0.0808319\tvalid_1's rmse: 0.0824551\n",
      "[2175]\ttraining's rmse: 0.0808299\tvalid_1's rmse: 0.0824546\n",
      "[2200]\ttraining's rmse: 0.080827\tvalid_1's rmse: 0.0824539\n",
      "[2225]\ttraining's rmse: 0.0808256\tvalid_1's rmse: 0.0824536\n",
      "[2250]\ttraining's rmse: 0.0808221\tvalid_1's rmse: 0.0824528\n",
      "[2275]\ttraining's rmse: 0.08082\tvalid_1's rmse: 0.0824524\n",
      "[2300]\ttraining's rmse: 0.0808176\tvalid_1's rmse: 0.0824519\n",
      "[2325]\ttraining's rmse: 0.0808147\tvalid_1's rmse: 0.0824514\n",
      "[2350]\ttraining's rmse: 0.0808124\tvalid_1's rmse: 0.0824509\n",
      "[2375]\ttraining's rmse: 0.0808099\tvalid_1's rmse: 0.0824508\n",
      "[2400]\ttraining's rmse: 0.0808079\tvalid_1's rmse: 0.0824503\n",
      "[2425]\ttraining's rmse: 0.0808059\tvalid_1's rmse: 0.0824501\n",
      "[2450]\ttraining's rmse: 0.0808034\tvalid_1's rmse: 0.0824495\n",
      "[2475]\ttraining's rmse: 0.0808009\tvalid_1's rmse: 0.0824492\n",
      "[2500]\ttraining's rmse: 0.0807981\tvalid_1's rmse: 0.0824488\n",
      "[2525]\ttraining's rmse: 0.0807965\tvalid_1's rmse: 0.0824486\n",
      "[2550]\ttraining's rmse: 0.0807941\tvalid_1's rmse: 0.0824484\n",
      "[2575]\ttraining's rmse: 0.0807924\tvalid_1's rmse: 0.0824481\n",
      "[2600]\ttraining's rmse: 0.0807909\tvalid_1's rmse: 0.0824476\n",
      "[2625]\ttraining's rmse: 0.0807888\tvalid_1's rmse: 0.0824473\n",
      "[2650]\ttraining's rmse: 0.0807878\tvalid_1's rmse: 0.0824471\n",
      "[2675]\ttraining's rmse: 0.0807865\tvalid_1's rmse: 0.0824468\n",
      "[2700]\ttraining's rmse: 0.0807855\tvalid_1's rmse: 0.0824466\n",
      "[2725]\ttraining's rmse: 0.0807834\tvalid_1's rmse: 0.0824458\n",
      "[2750]\ttraining's rmse: 0.0807808\tvalid_1's rmse: 0.0824456\n",
      "[2775]\ttraining's rmse: 0.0807786\tvalid_1's rmse: 0.0824454\n",
      "[2800]\ttraining's rmse: 0.080777\tvalid_1's rmse: 0.0824451\n",
      "[2825]\ttraining's rmse: 0.0807744\tvalid_1's rmse: 0.0824447\n",
      "[2850]\ttraining's rmse: 0.0807728\tvalid_1's rmse: 0.0824442\n",
      "[2875]\ttraining's rmse: 0.0807712\tvalid_1's rmse: 0.0824441\n",
      "[2900]\ttraining's rmse: 0.0807676\tvalid_1's rmse: 0.0824433\n",
      "[2925]\ttraining's rmse: 0.0807649\tvalid_1's rmse: 0.0824427\n",
      "[2950]\ttraining's rmse: 0.0807641\tvalid_1's rmse: 0.0824425\n",
      "[2975]\ttraining's rmse: 0.0807631\tvalid_1's rmse: 0.0824422\n",
      "[3000]\ttraining's rmse: 0.0807607\tvalid_1's rmse: 0.0824418\n",
      "[3025]\ttraining's rmse: 0.0807593\tvalid_1's rmse: 0.0824416\n",
      "[3050]\ttraining's rmse: 0.0807582\tvalid_1's rmse: 0.0824414\n",
      "[3075]\ttraining's rmse: 0.0807571\tvalid_1's rmse: 0.0824411\n",
      "[3100]\ttraining's rmse: 0.0807551\tvalid_1's rmse: 0.0824411\n",
      "[3125]\ttraining's rmse: 0.0807543\tvalid_1's rmse: 0.082441\n",
      "[3150]\ttraining's rmse: 0.0807517\tvalid_1's rmse: 0.082441\n",
      "[3175]\ttraining's rmse: 0.0807509\tvalid_1's rmse: 0.082441\n",
      "Early stopping, best iteration is:\n",
      "[3133]\ttraining's rmse: 0.0807528\tvalid_1's rmse: 0.0824407\n",
      "\n",
      "Training (ensemble): 3 ...\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[25]\ttraining's rmse: 0.0828748\tvalid_1's rmse: 0.0835276\n",
      "[50]\ttraining's rmse: 0.0827803\tvalid_1's rmse: 0.0834629\n",
      "[75]\ttraining's rmse: 0.0826955\tvalid_1's rmse: 0.0834045\n",
      "[100]\ttraining's rmse: 0.0826126\tvalid_1's rmse: 0.0833479\n",
      "[125]\ttraining's rmse: 0.0825392\tvalid_1's rmse: 0.0832972\n",
      "[150]\ttraining's rmse: 0.0824683\tvalid_1's rmse: 0.0832491\n",
      "[175]\ttraining's rmse: 0.0824025\tvalid_1's rmse: 0.0832043\n",
      "[200]\ttraining's rmse: 0.0823322\tvalid_1's rmse: 0.0831595\n",
      "[225]\ttraining's rmse: 0.0822692\tvalid_1's rmse: 0.0831196\n",
      "[250]\ttraining's rmse: 0.0822081\tvalid_1's rmse: 0.0830819\n",
      "[275]\ttraining's rmse: 0.0821507\tvalid_1's rmse: 0.0830468\n",
      "[300]\ttraining's rmse: 0.0820976\tvalid_1's rmse: 0.0830132\n",
      "[325]\ttraining's rmse: 0.0820419\tvalid_1's rmse: 0.0829796\n",
      "[350]\ttraining's rmse: 0.0819953\tvalid_1's rmse: 0.0829525\n",
      "[375]\ttraining's rmse: 0.0819515\tvalid_1's rmse: 0.0829258\n",
      "[400]\ttraining's rmse: 0.0819094\tvalid_1's rmse: 0.0829009\n",
      "[425]\ttraining's rmse: 0.0818636\tvalid_1's rmse: 0.0828778\n",
      "[450]\ttraining's rmse: 0.0818212\tvalid_1's rmse: 0.0828549\n",
      "[475]\ttraining's rmse: 0.0817819\tvalid_1's rmse: 0.0828334\n",
      "[500]\ttraining's rmse: 0.0817423\tvalid_1's rmse: 0.0828137\n",
      "[525]\ttraining's rmse: 0.0817076\tvalid_1's rmse: 0.0827951\n",
      "[550]\ttraining's rmse: 0.081673\tvalid_1's rmse: 0.0827771\n",
      "[575]\ttraining's rmse: 0.0816381\tvalid_1's rmse: 0.0827601\n",
      "[600]\ttraining's rmse: 0.0816115\tvalid_1's rmse: 0.0827452\n",
      "[625]\ttraining's rmse: 0.0815799\tvalid_1's rmse: 0.08273\n",
      "[650]\ttraining's rmse: 0.081552\tvalid_1's rmse: 0.082716\n",
      "[675]\ttraining's rmse: 0.0815216\tvalid_1's rmse: 0.082702\n",
      "[700]\ttraining's rmse: 0.0814909\tvalid_1's rmse: 0.0826878\n",
      "[725]\ttraining's rmse: 0.0814667\tvalid_1's rmse: 0.0826763\n",
      "[750]\ttraining's rmse: 0.081441\tvalid_1's rmse: 0.0826638\n",
      "[775]\ttraining's rmse: 0.0814155\tvalid_1's rmse: 0.0826515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[800]\ttraining's rmse: 0.0813934\tvalid_1's rmse: 0.082641\n",
      "[825]\ttraining's rmse: 0.0813672\tvalid_1's rmse: 0.0826311\n",
      "[850]\ttraining's rmse: 0.0813426\tvalid_1's rmse: 0.0826224\n",
      "[875]\ttraining's rmse: 0.0813209\tvalid_1's rmse: 0.0826147\n",
      "[900]\ttraining's rmse: 0.0812995\tvalid_1's rmse: 0.0826056\n",
      "[925]\ttraining's rmse: 0.0812796\tvalid_1's rmse: 0.0825979\n",
      "[950]\ttraining's rmse: 0.0812604\tvalid_1's rmse: 0.08259\n",
      "[975]\ttraining's rmse: 0.0812414\tvalid_1's rmse: 0.0825823\n",
      "[1000]\ttraining's rmse: 0.0812235\tvalid_1's rmse: 0.0825754\n",
      "[1025]\ttraining's rmse: 0.0812063\tvalid_1's rmse: 0.082569\n",
      "[1050]\ttraining's rmse: 0.0811913\tvalid_1's rmse: 0.0825628\n",
      "[1075]\ttraining's rmse: 0.0811708\tvalid_1's rmse: 0.0825571\n",
      "[1100]\ttraining's rmse: 0.0811568\tvalid_1's rmse: 0.082551\n",
      "[1125]\ttraining's rmse: 0.0811392\tvalid_1's rmse: 0.0825457\n",
      "[1150]\ttraining's rmse: 0.0811239\tvalid_1's rmse: 0.0825413\n",
      "[1175]\ttraining's rmse: 0.0811116\tvalid_1's rmse: 0.0825362\n",
      "[1200]\ttraining's rmse: 0.0810981\tvalid_1's rmse: 0.0825318\n",
      "[1225]\ttraining's rmse: 0.0810846\tvalid_1's rmse: 0.0825268\n",
      "[1250]\ttraining's rmse: 0.0810723\tvalid_1's rmse: 0.0825226\n",
      "[1275]\ttraining's rmse: 0.0810604\tvalid_1's rmse: 0.0825184\n",
      "[1300]\ttraining's rmse: 0.0810486\tvalid_1's rmse: 0.0825144\n",
      "[1325]\ttraining's rmse: 0.0810377\tvalid_1's rmse: 0.0825109\n",
      "[1350]\ttraining's rmse: 0.0810255\tvalid_1's rmse: 0.0825072\n",
      "[1375]\ttraining's rmse: 0.0810157\tvalid_1's rmse: 0.0825042\n",
      "[1400]\ttraining's rmse: 0.081004\tvalid_1's rmse: 0.0825015\n",
      "[1425]\ttraining's rmse: 0.0809948\tvalid_1's rmse: 0.0824994\n",
      "[1450]\ttraining's rmse: 0.0809839\tvalid_1's rmse: 0.0824966\n",
      "[1475]\ttraining's rmse: 0.0809754\tvalid_1's rmse: 0.0824929\n",
      "[1500]\ttraining's rmse: 0.0809672\tvalid_1's rmse: 0.0824898\n",
      "[1525]\ttraining's rmse: 0.0809584\tvalid_1's rmse: 0.0824874\n",
      "[1550]\ttraining's rmse: 0.0809487\tvalid_1's rmse: 0.082485\n",
      "[1575]\ttraining's rmse: 0.0809387\tvalid_1's rmse: 0.0824827\n",
      "[1600]\ttraining's rmse: 0.080934\tvalid_1's rmse: 0.0824812\n",
      "[1625]\ttraining's rmse: 0.0809267\tvalid_1's rmse: 0.0824788\n",
      "[1650]\ttraining's rmse: 0.0809201\tvalid_1's rmse: 0.0824768\n",
      "[1675]\ttraining's rmse: 0.0809138\tvalid_1's rmse: 0.0824756\n",
      "[1700]\ttraining's rmse: 0.0809074\tvalid_1's rmse: 0.0824734\n",
      "[1725]\ttraining's rmse: 0.0808994\tvalid_1's rmse: 0.0824713\n",
      "[1750]\ttraining's rmse: 0.0808932\tvalid_1's rmse: 0.0824699\n",
      "[1775]\ttraining's rmse: 0.0808873\tvalid_1's rmse: 0.0824685\n",
      "[1800]\ttraining's rmse: 0.0808809\tvalid_1's rmse: 0.0824668\n",
      "[1825]\ttraining's rmse: 0.0808755\tvalid_1's rmse: 0.0824656\n",
      "[1850]\ttraining's rmse: 0.0808713\tvalid_1's rmse: 0.0824644\n",
      "[1875]\ttraining's rmse: 0.0808675\tvalid_1's rmse: 0.0824632\n",
      "[1900]\ttraining's rmse: 0.0808637\tvalid_1's rmse: 0.0824623\n",
      "[1925]\ttraining's rmse: 0.0808583\tvalid_1's rmse: 0.0824608\n",
      "[1950]\ttraining's rmse: 0.0808535\tvalid_1's rmse: 0.0824602\n",
      "[1975]\ttraining's rmse: 0.0808487\tvalid_1's rmse: 0.0824594\n",
      "[2000]\ttraining's rmse: 0.0808451\tvalid_1's rmse: 0.0824583\n",
      "[2025]\ttraining's rmse: 0.0808419\tvalid_1's rmse: 0.0824574\n",
      "[2050]\ttraining's rmse: 0.080838\tvalid_1's rmse: 0.0824561\n",
      "[2075]\ttraining's rmse: 0.0808342\tvalid_1's rmse: 0.0824556\n",
      "[2100]\ttraining's rmse: 0.080831\tvalid_1's rmse: 0.082455\n",
      "[2125]\ttraining's rmse: 0.0808291\tvalid_1's rmse: 0.0824543\n",
      "[2150]\ttraining's rmse: 0.0808262\tvalid_1's rmse: 0.0824539\n",
      "[2175]\ttraining's rmse: 0.0808228\tvalid_1's rmse: 0.0824535\n",
      "[2200]\ttraining's rmse: 0.0808198\tvalid_1's rmse: 0.0824525\n",
      "[2225]\ttraining's rmse: 0.0808176\tvalid_1's rmse: 0.0824519\n",
      "[2250]\ttraining's rmse: 0.0808143\tvalid_1's rmse: 0.0824514\n",
      "[2275]\ttraining's rmse: 0.0808114\tvalid_1's rmse: 0.0824507\n",
      "[2300]\ttraining's rmse: 0.0808092\tvalid_1's rmse: 0.0824504\n",
      "[2325]\ttraining's rmse: 0.0808053\tvalid_1's rmse: 0.0824492\n",
      "[2350]\ttraining's rmse: 0.0808033\tvalid_1's rmse: 0.0824488\n",
      "[2375]\ttraining's rmse: 0.0808011\tvalid_1's rmse: 0.0824483\n",
      "[2400]\ttraining's rmse: 0.0807995\tvalid_1's rmse: 0.0824477\n",
      "[2425]\ttraining's rmse: 0.080798\tvalid_1's rmse: 0.0824472\n",
      "[2450]\ttraining's rmse: 0.0807953\tvalid_1's rmse: 0.0824468\n",
      "[2475]\ttraining's rmse: 0.080794\tvalid_1's rmse: 0.0824464\n",
      "[2500]\ttraining's rmse: 0.080792\tvalid_1's rmse: 0.0824459\n",
      "[2525]\ttraining's rmse: 0.0807898\tvalid_1's rmse: 0.0824453\n",
      "[2550]\ttraining's rmse: 0.0807883\tvalid_1's rmse: 0.0824446\n",
      "[2575]\ttraining's rmse: 0.0807856\tvalid_1's rmse: 0.082444\n",
      "[2600]\ttraining's rmse: 0.0807847\tvalid_1's rmse: 0.0824438\n",
      "[2625]\ttraining's rmse: 0.0807835\tvalid_1's rmse: 0.0824436\n",
      "[2650]\ttraining's rmse: 0.0807821\tvalid_1's rmse: 0.0824432\n",
      "[2675]\ttraining's rmse: 0.0807808\tvalid_1's rmse: 0.0824433\n",
      "[2700]\ttraining's rmse: 0.0807793\tvalid_1's rmse: 0.0824433\n",
      "[2725]\ttraining's rmse: 0.0807768\tvalid_1's rmse: 0.082443\n",
      "[2750]\ttraining's rmse: 0.0807751\tvalid_1's rmse: 0.0824428\n",
      "[2775]\ttraining's rmse: 0.080774\tvalid_1's rmse: 0.0824424\n",
      "[2800]\ttraining's rmse: 0.080773\tvalid_1's rmse: 0.0824424\n",
      "[2825]\ttraining's rmse: 0.0807716\tvalid_1's rmse: 0.0824423\n",
      "[2850]\ttraining's rmse: 0.0807705\tvalid_1's rmse: 0.0824419\n",
      "[2875]\ttraining's rmse: 0.0807689\tvalid_1's rmse: 0.0824417\n",
      "[2900]\ttraining's rmse: 0.0807677\tvalid_1's rmse: 0.0824416\n",
      "[2925]\ttraining's rmse: 0.0807667\tvalid_1's rmse: 0.0824412\n",
      "[2950]\ttraining's rmse: 0.0807659\tvalid_1's rmse: 0.0824412\n",
      "[2975]\ttraining's rmse: 0.0807647\tvalid_1's rmse: 0.0824413\n",
      "Early stopping, best iteration is:\n",
      "[2928]\ttraining's rmse: 0.0807665\tvalid_1's rmse: 0.0824411\n",
      "\n",
      "Training (ensemble): 4 ...\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[25]\ttraining's rmse: 0.0828855\tvalid_1's rmse: 0.0835347\n",
      "[50]\ttraining's rmse: 0.0827902\tvalid_1's rmse: 0.0834678\n",
      "[75]\ttraining's rmse: 0.0827032\tvalid_1's rmse: 0.0834068\n",
      "[100]\ttraining's rmse: 0.0826171\tvalid_1's rmse: 0.0833493\n",
      "[125]\ttraining's rmse: 0.0825327\tvalid_1's rmse: 0.0832942\n",
      "[150]\ttraining's rmse: 0.082458\tvalid_1's rmse: 0.0832459\n",
      "[175]\ttraining's rmse: 0.0823882\tvalid_1's rmse: 0.0832007\n",
      "[200]\ttraining's rmse: 0.0823189\tvalid_1's rmse: 0.0831564\n",
      "[225]\ttraining's rmse: 0.0822582\tvalid_1's rmse: 0.0831173\n",
      "[250]\ttraining's rmse: 0.0822003\tvalid_1's rmse: 0.0830809\n",
      "[275]\ttraining's rmse: 0.0821448\tvalid_1's rmse: 0.0830469\n",
      "[300]\ttraining's rmse: 0.0820888\tvalid_1's rmse: 0.0830138\n",
      "[325]\ttraining's rmse: 0.0820362\tvalid_1's rmse: 0.0829832\n",
      "[350]\ttraining's rmse: 0.0819888\tvalid_1's rmse: 0.0829549\n",
      "[375]\ttraining's rmse: 0.0819416\tvalid_1's rmse: 0.0829278\n",
      "[400]\ttraining's rmse: 0.0818974\tvalid_1's rmse: 0.0829037\n",
      "[425]\ttraining's rmse: 0.081859\tvalid_1's rmse: 0.0828817\n",
      "[450]\ttraining's rmse: 0.0818222\tvalid_1's rmse: 0.08286\n",
      "[475]\ttraining's rmse: 0.0817841\tvalid_1's rmse: 0.0828395\n",
      "[500]\ttraining's rmse: 0.0817489\tvalid_1's rmse: 0.0828204\n",
      "[525]\ttraining's rmse: 0.0817125\tvalid_1's rmse: 0.0828016\n",
      "[550]\ttraining's rmse: 0.0816767\tvalid_1's rmse: 0.0827825\n",
      "[575]\ttraining's rmse: 0.0816459\tvalid_1's rmse: 0.0827638\n",
      "[600]\ttraining's rmse: 0.0816152\tvalid_1's rmse: 0.082747\n",
      "[625]\ttraining's rmse: 0.0815876\tvalid_1's rmse: 0.0827326\n",
      "[650]\ttraining's rmse: 0.0815589\tvalid_1's rmse: 0.0827182\n",
      "[675]\ttraining's rmse: 0.0815315\tvalid_1's rmse: 0.0827052\n",
      "[700]\ttraining's rmse: 0.0815063\tvalid_1's rmse: 0.082692\n",
      "[725]\ttraining's rmse: 0.0814813\tvalid_1's rmse: 0.0826797\n",
      "[750]\ttraining's rmse: 0.0814528\tvalid_1's rmse: 0.0826677\n",
      "[775]\ttraining's rmse: 0.0814298\tvalid_1's rmse: 0.0826569\n",
      "[800]\ttraining's rmse: 0.0814084\tvalid_1's rmse: 0.0826466\n",
      "[825]\ttraining's rmse: 0.0813825\tvalid_1's rmse: 0.0826372\n",
      "[850]\ttraining's rmse: 0.081357\tvalid_1's rmse: 0.0826287\n",
      "[875]\ttraining's rmse: 0.0813343\tvalid_1's rmse: 0.0826197\n",
      "[900]\ttraining's rmse: 0.0813141\tvalid_1's rmse: 0.0826103\n",
      "[925]\ttraining's rmse: 0.0812931\tvalid_1's rmse: 0.082602\n",
      "[950]\ttraining's rmse: 0.0812771\tvalid_1's rmse: 0.0825947\n",
      "[975]\ttraining's rmse: 0.0812562\tvalid_1's rmse: 0.0825867\n",
      "[1000]\ttraining's rmse: 0.0812389\tvalid_1's rmse: 0.0825801\n",
      "[1025]\ttraining's rmse: 0.0812226\tvalid_1's rmse: 0.0825742\n",
      "[1050]\ttraining's rmse: 0.0812043\tvalid_1's rmse: 0.0825676\n",
      "[1075]\ttraining's rmse: 0.0811847\tvalid_1's rmse: 0.0825617\n",
      "[1100]\ttraining's rmse: 0.0811679\tvalid_1's rmse: 0.0825565\n",
      "[1125]\ttraining's rmse: 0.0811512\tvalid_1's rmse: 0.0825513\n",
      "[1150]\ttraining's rmse: 0.0811346\tvalid_1's rmse: 0.0825462\n",
      "[1175]\ttraining's rmse: 0.081121\tvalid_1's rmse: 0.0825414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1200]\ttraining's rmse: 0.0811057\tvalid_1's rmse: 0.0825358\n",
      "[1225]\ttraining's rmse: 0.0810938\tvalid_1's rmse: 0.0825313\n",
      "[1250]\ttraining's rmse: 0.0810802\tvalid_1's rmse: 0.0825265\n",
      "[1275]\ttraining's rmse: 0.0810673\tvalid_1's rmse: 0.0825224\n",
      "[1300]\ttraining's rmse: 0.0810567\tvalid_1's rmse: 0.0825179\n",
      "[1325]\ttraining's rmse: 0.0810449\tvalid_1's rmse: 0.0825146\n",
      "[1350]\ttraining's rmse: 0.0810342\tvalid_1's rmse: 0.0825114\n",
      "[1375]\ttraining's rmse: 0.0810233\tvalid_1's rmse: 0.0825083\n",
      "[1400]\ttraining's rmse: 0.0810129\tvalid_1's rmse: 0.0825046\n",
      "[1425]\ttraining's rmse: 0.081002\tvalid_1's rmse: 0.0825022\n",
      "[1450]\ttraining's rmse: 0.0809893\tvalid_1's rmse: 0.0824994\n",
      "[1475]\ttraining's rmse: 0.0809798\tvalid_1's rmse: 0.0824966\n",
      "[1500]\ttraining's rmse: 0.0809709\tvalid_1's rmse: 0.0824944\n",
      "[1525]\ttraining's rmse: 0.0809613\tvalid_1's rmse: 0.0824917\n",
      "[1550]\ttraining's rmse: 0.0809543\tvalid_1's rmse: 0.0824897\n",
      "[1575]\ttraining's rmse: 0.0809454\tvalid_1's rmse: 0.0824879\n",
      "[1600]\ttraining's rmse: 0.0809375\tvalid_1's rmse: 0.0824863\n",
      "[1625]\ttraining's rmse: 0.0809286\tvalid_1's rmse: 0.0824839\n",
      "[1650]\ttraining's rmse: 0.0809227\tvalid_1's rmse: 0.0824822\n",
      "[1675]\ttraining's rmse: 0.0809153\tvalid_1's rmse: 0.0824803\n",
      "[1700]\ttraining's rmse: 0.0809102\tvalid_1's rmse: 0.0824786\n",
      "[1725]\ttraining's rmse: 0.0809043\tvalid_1's rmse: 0.0824773\n",
      "[1750]\ttraining's rmse: 0.0808978\tvalid_1's rmse: 0.082476\n",
      "[1775]\ttraining's rmse: 0.0808906\tvalid_1's rmse: 0.0824744\n",
      "[1800]\ttraining's rmse: 0.0808852\tvalid_1's rmse: 0.0824731\n",
      "[1825]\ttraining's rmse: 0.0808799\tvalid_1's rmse: 0.0824717\n",
      "[1850]\ttraining's rmse: 0.0808753\tvalid_1's rmse: 0.0824703\n",
      "[1875]\ttraining's rmse: 0.0808712\tvalid_1's rmse: 0.0824693\n",
      "[1900]\ttraining's rmse: 0.0808669\tvalid_1's rmse: 0.0824685\n",
      "[1925]\ttraining's rmse: 0.080864\tvalid_1's rmse: 0.0824677\n",
      "[1950]\ttraining's rmse: 0.0808609\tvalid_1's rmse: 0.0824671\n",
      "[1975]\ttraining's rmse: 0.0808566\tvalid_1's rmse: 0.0824666\n",
      "[2000]\ttraining's rmse: 0.0808521\tvalid_1's rmse: 0.0824656\n",
      "[2025]\ttraining's rmse: 0.0808471\tvalid_1's rmse: 0.0824643\n",
      "[2050]\ttraining's rmse: 0.0808433\tvalid_1's rmse: 0.0824638\n",
      "[2075]\ttraining's rmse: 0.0808401\tvalid_1's rmse: 0.0824631\n",
      "[2100]\ttraining's rmse: 0.0808366\tvalid_1's rmse: 0.0824617\n",
      "[2125]\ttraining's rmse: 0.0808328\tvalid_1's rmse: 0.0824605\n",
      "[2150]\ttraining's rmse: 0.0808294\tvalid_1's rmse: 0.0824599\n",
      "[2175]\ttraining's rmse: 0.0808266\tvalid_1's rmse: 0.082459\n",
      "[2200]\ttraining's rmse: 0.0808238\tvalid_1's rmse: 0.0824586\n",
      "[2225]\ttraining's rmse: 0.080822\tvalid_1's rmse: 0.0824583\n",
      "[2250]\ttraining's rmse: 0.0808199\tvalid_1's rmse: 0.0824577\n",
      "[2275]\ttraining's rmse: 0.080817\tvalid_1's rmse: 0.0824567\n",
      "[2300]\ttraining's rmse: 0.0808139\tvalid_1's rmse: 0.0824559\n",
      "[2325]\ttraining's rmse: 0.0808108\tvalid_1's rmse: 0.0824553\n",
      "[2350]\ttraining's rmse: 0.0808084\tvalid_1's rmse: 0.0824549\n",
      "[2375]\ttraining's rmse: 0.0808054\tvalid_1's rmse: 0.0824545\n",
      "[2400]\ttraining's rmse: 0.0808034\tvalid_1's rmse: 0.0824539\n",
      "[2425]\ttraining's rmse: 0.0808008\tvalid_1's rmse: 0.0824538\n",
      "[2450]\ttraining's rmse: 0.0807995\tvalid_1's rmse: 0.0824534\n",
      "[2475]\ttraining's rmse: 0.0807974\tvalid_1's rmse: 0.082453\n",
      "[2500]\ttraining's rmse: 0.0807947\tvalid_1's rmse: 0.0824528\n",
      "[2525]\ttraining's rmse: 0.0807925\tvalid_1's rmse: 0.0824522\n",
      "[2550]\ttraining's rmse: 0.0807903\tvalid_1's rmse: 0.0824517\n",
      "[2575]\ttraining's rmse: 0.0807879\tvalid_1's rmse: 0.0824514\n",
      "[2600]\ttraining's rmse: 0.0807868\tvalid_1's rmse: 0.0824511\n",
      "[2625]\ttraining's rmse: 0.0807855\tvalid_1's rmse: 0.0824507\n",
      "[2650]\ttraining's rmse: 0.0807839\tvalid_1's rmse: 0.0824508\n",
      "[2675]\ttraining's rmse: 0.0807828\tvalid_1's rmse: 0.0824509\n",
      "[2700]\ttraining's rmse: 0.0807812\tvalid_1's rmse: 0.0824504\n",
      "[2725]\ttraining's rmse: 0.0807786\tvalid_1's rmse: 0.0824498\n",
      "[2750]\ttraining's rmse: 0.0807758\tvalid_1's rmse: 0.0824494\n",
      "[2775]\ttraining's rmse: 0.0807742\tvalid_1's rmse: 0.0824491\n",
      "[2800]\ttraining's rmse: 0.0807719\tvalid_1's rmse: 0.0824484\n",
      "[2825]\ttraining's rmse: 0.0807703\tvalid_1's rmse: 0.0824481\n",
      "[2850]\ttraining's rmse: 0.0807692\tvalid_1's rmse: 0.0824481\n",
      "[2875]\ttraining's rmse: 0.0807679\tvalid_1's rmse: 0.0824481\n",
      "[2900]\ttraining's rmse: 0.0807667\tvalid_1's rmse: 0.0824476\n",
      "[2925]\ttraining's rmse: 0.0807653\tvalid_1's rmse: 0.0824474\n",
      "[2950]\ttraining's rmse: 0.0807638\tvalid_1's rmse: 0.0824471\n",
      "[2975]\ttraining's rmse: 0.0807623\tvalid_1's rmse: 0.0824468\n",
      "[3000]\ttraining's rmse: 0.0807611\tvalid_1's rmse: 0.0824465\n",
      "[3025]\ttraining's rmse: 0.08076\tvalid_1's rmse: 0.0824464\n",
      "[3050]\ttraining's rmse: 0.0807592\tvalid_1's rmse: 0.0824463\n",
      "[3075]\ttraining's rmse: 0.0807582\tvalid_1's rmse: 0.0824465\n",
      "[3100]\ttraining's rmse: 0.0807579\tvalid_1's rmse: 0.0824463\n",
      "[3125]\ttraining's rmse: 0.0807564\tvalid_1's rmse: 0.082446\n",
      "[3150]\ttraining's rmse: 0.080755\tvalid_1's rmse: 0.0824457\n",
      "[3175]\ttraining's rmse: 0.0807529\tvalid_1's rmse: 0.0824455\n",
      "[3200]\ttraining's rmse: 0.0807526\tvalid_1's rmse: 0.0824454\n",
      "[3225]\ttraining's rmse: 0.0807517\tvalid_1's rmse: 0.0824451\n",
      "[3250]\ttraining's rmse: 0.0807504\tvalid_1's rmse: 0.0824451\n",
      "[3275]\ttraining's rmse: 0.0807491\tvalid_1's rmse: 0.082445\n",
      "[3300]\ttraining's rmse: 0.0807477\tvalid_1's rmse: 0.0824448\n",
      "[3325]\ttraining's rmse: 0.0807462\tvalid_1's rmse: 0.0824446\n",
      "[3350]\ttraining's rmse: 0.0807439\tvalid_1's rmse: 0.0824444\n",
      "[3375]\ttraining's rmse: 0.0807426\tvalid_1's rmse: 0.0824445\n",
      "[3400]\ttraining's rmse: 0.0807418\tvalid_1's rmse: 0.0824442\n",
      "[3425]\ttraining's rmse: 0.0807406\tvalid_1's rmse: 0.0824442\n",
      "[3450]\ttraining's rmse: 0.0807398\tvalid_1's rmse: 0.082444\n",
      "[3475]\ttraining's rmse: 0.080739\tvalid_1's rmse: 0.0824439\n",
      "[3500]\ttraining's rmse: 0.0807386\tvalid_1's rmse: 0.0824439\n",
      "[3525]\ttraining's rmse: 0.0807383\tvalid_1's rmse: 0.0824439\n",
      "[3550]\ttraining's rmse: 0.0807367\tvalid_1's rmse: 0.0824437\n",
      "[3575]\ttraining's rmse: 0.0807358\tvalid_1's rmse: 0.0824435\n",
      "[3600]\ttraining's rmse: 0.0807344\tvalid_1's rmse: 0.0824431\n",
      "[3625]\ttraining's rmse: 0.0807336\tvalid_1's rmse: 0.0824428\n",
      "[3650]\ttraining's rmse: 0.0807324\tvalid_1's rmse: 0.0824424\n",
      "[3675]\ttraining's rmse: 0.0807319\tvalid_1's rmse: 0.0824423\n",
      "[3700]\ttraining's rmse: 0.0807306\tvalid_1's rmse: 0.0824422\n",
      "[3725]\ttraining's rmse: 0.0807297\tvalid_1's rmse: 0.0824422\n",
      "[3750]\ttraining's rmse: 0.080729\tvalid_1's rmse: 0.0824419\n",
      "[3775]\ttraining's rmse: 0.0807278\tvalid_1's rmse: 0.0824415\n",
      "[3800]\ttraining's rmse: 0.0807272\tvalid_1's rmse: 0.0824416\n",
      "[3825]\ttraining's rmse: 0.0807258\tvalid_1's rmse: 0.0824415\n",
      "[3850]\ttraining's rmse: 0.0807254\tvalid_1's rmse: 0.0824412\n",
      "[3875]\ttraining's rmse: 0.0807247\tvalid_1's rmse: 0.0824409\n",
      "[3900]\ttraining's rmse: 0.080724\tvalid_1's rmse: 0.0824407\n",
      "[3925]\ttraining's rmse: 0.0807236\tvalid_1's rmse: 0.0824406\n",
      "[3950]\ttraining's rmse: 0.0807233\tvalid_1's rmse: 0.0824406\n",
      "[3975]\ttraining's rmse: 0.0807227\tvalid_1's rmse: 0.0824408\n",
      "Early stopping, best iteration is:\n",
      "[3939]\ttraining's rmse: 0.0807234\tvalid_1's rmse: 0.0824405\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "score = 0\n",
    "models = []\n",
    "params = {\"objective\": \"regression\", \"boosting\": \"gbdt\", \"num_leaves\": 500, \n",
    "              \"learning_rate\": 0.002515, 'bagging_fraction': 0.8297, \"reg_lambda\": 0.1052, \n",
    "              'reg_alpha':0.1046, \"metric\": \"rmse\", 'max_depth': 10, 'min_child_weight': 27,\n",
    "              'verbose': -1, 'min_split_gain':0.08138 , 'subsample_freq':1, 'sub_feature':  0.4492}\n",
    "num_ensembles = 5\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=99)\n",
    "# ensemble models\n",
    "for i in range(num_ensembles):\n",
    "    print(\"\\nTraining (ensemble): %d ...\" % (i))\n",
    "    params['random_seed'] = i\n",
    "    d_training = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features, free_raw_data=False)\n",
    "    d_test = lgb.Dataset(X_test, label=y_test, categorical_feature=categorical_features, free_raw_data=False)\n",
    "    \n",
    "    model = lgb.train(params, train_set=d_training, num_boost_round=5000, valid_sets=[d_training,d_test],\n",
    "                      verbose_eval=25, early_stopping_rounds=50)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1087.64 MB\n",
      "Memory usage after optimization is: 428.55 MB\n",
      "Decreased by 60.6%\n",
      "Memory usage of dataframe is 1227.28 MB\n",
      "Memory usage after optimization is: 483.58 MB\n",
      "Decreased by 60.6%\n"
     ]
    }
   ],
   "source": [
    "df_sub_2016 = reduce_mem_usage(pd.read_csv('../output/final_sub_2016.csv')).drop_duplicates('parcelid')\n",
    "df_sub_2016['year'] = 0\n",
    "results = {}\n",
    "for month in [10,11,12]:\n",
    "    df_sub_2016['month'] = month\n",
    "    for i in models:\n",
    "        if  month not in results.keys():\n",
    "            results[month] = i.predict(df_sub_2016[list(features.columns)], \n",
    "                                       num_iteration=i.best_iteration) / len(models)\n",
    "        else:\n",
    "            results[month] += i.predict(df_sub_2016[list(features.columns)], \n",
    "                                        num_iteration=i.best_iteration) / len(models)\n",
    "for i in results.keys():\n",
    "    df_sub_2016[str(i)] = results[i]\n",
    "df_sub = pd.read_csv('../Resources/sample_submission.csv')\n",
    "df_sub = df_sub.rename(columns = {'ParcelId': 'parcelid'})\n",
    "df_sub = pd.merge(df_sub[['parcelid']], df_sub_2016[['parcelid', '10', '11', '12']].drop_duplicates('parcelid'),\n",
    "                  how = 'left', on = 'parcelid')\n",
    "df_sub = df_sub.rename(columns = {'10': '201610', '11': '201611', '12': '201612'}).drop_duplicates('parcelid')\n",
    "del df_sub_2016\n",
    "gc.collect()\n",
    "df_sub_2017 = reduce_mem_usage(pd.read_csv('../output/final_sub_2017.csv'))\n",
    "df_sub_2017['year'] = 1\n",
    "results = {}\n",
    "for month in [10,11,12]:\n",
    "    df_sub_2017['month'] = month\n",
    "    for i in models:\n",
    "        if  month not in results.keys():\n",
    "            results[month] = i.predict(df_sub_2017[list(features.columns)], \n",
    "                                       num_iteration=i.best_iteration) / len(models)\n",
    "        else:\n",
    "            results[month] += i.predict(df_sub_2017[list(features.columns)], \n",
    "                                        num_iteration=i.best_iteration) / len(models)\n",
    "for i in results.keys():\n",
    "    df_sub_2017[str(i)] = results[i]\n",
    "df_sub = pd.merge(df_sub[['parcelid', '201610', '201611', '201612']], \n",
    "                  df_sub_2017[['parcelid', '10', '11', '12']].drop_duplicates('parcelid'), \n",
    "                  how = 'left', on = 'parcelid')\n",
    "df_sub = df_sub.rename(columns = {'10': '201710', '11': '201711', '12': '201712'})\n",
    "del df_sub_2017\n",
    "gc.collect()\n",
    "df_sub.to_csv('../output/submission/lgb_en5_opt.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
