{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "\n",
    "def reduce_mem_usage(df, use_float16=False):\n",
    "    \"\"\"\n",
    "    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    \n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n",
    "            continue\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
    "    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 65.85 MB\n",
      "Memory usage after optimization is: 26.09 MB\n",
      "Decreased by 60.4%\n"
     ]
    }
   ],
   "source": [
    "df_merge = reduce_mem_usage(pd.read_csv('./output/final_merge.csv'))\n",
    "categorical_features = ['airconditioningtypeid', 'hashottuborspa', 'heatingorsystemtypeid', \n",
    "                       'pooltypeid2', 'propertylandusetypeid', 'fips', 'regionidcounty', \n",
    "                       'buildingqualitytypeid_fill', 'regionidcity_fill', 'year', \n",
    "                       'regionidneighborhood_fill', 'taxdelinquencyflag']\n",
    "df_drop = df_merge.drop_duplicates(subset = ['parcelid', 'logerror'])\n",
    "df_drop = df_drop[ df_drop.logerror > -0.4 ]\n",
    "df_drop = df_drop[ df_drop.logerror < 0.419 ]\n",
    "df_drop = df_drop.reset_index(drop = True)\n",
    "target = df_drop.logerror\n",
    "features = df_drop.drop(['logerror'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop.to_csv('./output/outlier_remove.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/home/ningzesun/.local/lib/python3.6/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[25]\ttraining's rmse: 0.0945452\tvalid_1's rmse: 0.0966607\n",
      "[50]\ttraining's rmse: 0.0944052\tvalid_1's rmse: 0.0965747\n",
      "[75]\ttraining's rmse: 0.0942687\tvalid_1's rmse: 0.0964955\n",
      "[100]\ttraining's rmse: 0.0941411\tvalid_1's rmse: 0.0964239\n",
      "[125]\ttraining's rmse: 0.094022\tvalid_1's rmse: 0.096354\n",
      "[150]\ttraining's rmse: 0.0939129\tvalid_1's rmse: 0.0962911\n",
      "[175]\ttraining's rmse: 0.0938204\tvalid_1's rmse: 0.0962349\n",
      "[200]\ttraining's rmse: 0.0937221\tvalid_1's rmse: 0.096183\n",
      "[225]\ttraining's rmse: 0.0936282\tvalid_1's rmse: 0.0961359\n",
      "[250]\ttraining's rmse: 0.0935509\tvalid_1's rmse: 0.0960918\n",
      "[275]\ttraining's rmse: 0.0934673\tvalid_1's rmse: 0.0960499\n",
      "[300]\ttraining's rmse: 0.0933895\tvalid_1's rmse: 0.0960122\n",
      "[325]\ttraining's rmse: 0.0933174\tvalid_1's rmse: 0.0959781\n",
      "[350]\ttraining's rmse: 0.0932435\tvalid_1's rmse: 0.0959464\n",
      "[375]\ttraining's rmse: 0.0931813\tvalid_1's rmse: 0.09592\n",
      "[400]\ttraining's rmse: 0.0931184\tvalid_1's rmse: 0.095895\n",
      "[425]\ttraining's rmse: 0.0930608\tvalid_1's rmse: 0.0958688\n",
      "[450]\ttraining's rmse: 0.0930067\tvalid_1's rmse: 0.0958449\n",
      "[475]\ttraining's rmse: 0.0929594\tvalid_1's rmse: 0.0958217\n",
      "[500]\ttraining's rmse: 0.0929141\tvalid_1's rmse: 0.0958003\n",
      "[525]\ttraining's rmse: 0.0928621\tvalid_1's rmse: 0.0957808\n",
      "[550]\ttraining's rmse: 0.0928083\tvalid_1's rmse: 0.0957637\n",
      "[575]\ttraining's rmse: 0.092761\tvalid_1's rmse: 0.095749\n",
      "[600]\ttraining's rmse: 0.0927117\tvalid_1's rmse: 0.0957334\n",
      "[625]\ttraining's rmse: 0.0926739\tvalid_1's rmse: 0.0957186\n",
      "[650]\ttraining's rmse: 0.0926308\tvalid_1's rmse: 0.0957043\n",
      "[675]\ttraining's rmse: 0.0925878\tvalid_1's rmse: 0.0956901\n",
      "[700]\ttraining's rmse: 0.0925494\tvalid_1's rmse: 0.0956767\n",
      "[725]\ttraining's rmse: 0.0925196\tvalid_1's rmse: 0.0956668\n",
      "[750]\ttraining's rmse: 0.0924874\tvalid_1's rmse: 0.0956573\n",
      "[775]\ttraining's rmse: 0.0924595\tvalid_1's rmse: 0.0956476\n",
      "[800]\ttraining's rmse: 0.0924299\tvalid_1's rmse: 0.0956391\n",
      "[825]\ttraining's rmse: 0.0924035\tvalid_1's rmse: 0.0956295\n",
      "[850]\ttraining's rmse: 0.0923744\tvalid_1's rmse: 0.0956191\n",
      "[875]\ttraining's rmse: 0.0923481\tvalid_1's rmse: 0.0956108\n",
      "[900]\ttraining's rmse: 0.0923206\tvalid_1's rmse: 0.095602\n",
      "[925]\ttraining's rmse: 0.0922984\tvalid_1's rmse: 0.0955951\n",
      "[950]\ttraining's rmse: 0.0922771\tvalid_1's rmse: 0.0955882\n",
      "[975]\ttraining's rmse: 0.0922517\tvalid_1's rmse: 0.0955805\n",
      "[1000]\ttraining's rmse: 0.0922312\tvalid_1's rmse: 0.0955725\n",
      "[1025]\ttraining's rmse: 0.0922067\tvalid_1's rmse: 0.0955661\n",
      "[1050]\ttraining's rmse: 0.0921885\tvalid_1's rmse: 0.0955603\n",
      "[1075]\ttraining's rmse: 0.0921673\tvalid_1's rmse: 0.095555\n",
      "[1100]\ttraining's rmse: 0.0921525\tvalid_1's rmse: 0.0955513\n",
      "[1125]\ttraining's rmse: 0.0921359\tvalid_1's rmse: 0.0955459\n",
      "[1150]\ttraining's rmse: 0.0921164\tvalid_1's rmse: 0.095542\n",
      "[1175]\ttraining's rmse: 0.0920991\tvalid_1's rmse: 0.0955383\n",
      "[1200]\ttraining's rmse: 0.0920829\tvalid_1's rmse: 0.0955337\n",
      "[1225]\ttraining's rmse: 0.0920685\tvalid_1's rmse: 0.0955307\n",
      "[1250]\ttraining's rmse: 0.0920577\tvalid_1's rmse: 0.0955269\n",
      "[1275]\ttraining's rmse: 0.092044\tvalid_1's rmse: 0.0955235\n",
      "[1300]\ttraining's rmse: 0.0920306\tvalid_1's rmse: 0.0955204\n",
      "[1325]\ttraining's rmse: 0.0920187\tvalid_1's rmse: 0.0955179\n",
      "[1350]\ttraining's rmse: 0.0920058\tvalid_1's rmse: 0.0955138\n",
      "[1375]\ttraining's rmse: 0.0919924\tvalid_1's rmse: 0.0955103\n",
      "[1400]\ttraining's rmse: 0.0919826\tvalid_1's rmse: 0.0955068\n",
      "[1425]\ttraining's rmse: 0.0919721\tvalid_1's rmse: 0.0955043\n",
      "[1450]\ttraining's rmse: 0.0919631\tvalid_1's rmse: 0.0955014\n",
      "[1475]\ttraining's rmse: 0.0919553\tvalid_1's rmse: 0.095499\n",
      "[1500]\ttraining's rmse: 0.091947\tvalid_1's rmse: 0.0954956\n",
      "[1525]\ttraining's rmse: 0.0919356\tvalid_1's rmse: 0.0954915\n",
      "[1550]\ttraining's rmse: 0.0919222\tvalid_1's rmse: 0.0954899\n",
      "[1575]\ttraining's rmse: 0.0919151\tvalid_1's rmse: 0.0954885\n",
      "[1600]\ttraining's rmse: 0.0919052\tvalid_1's rmse: 0.0954862\n",
      "[1625]\ttraining's rmse: 0.091897\tvalid_1's rmse: 0.0954837\n",
      "[1650]\ttraining's rmse: 0.0918902\tvalid_1's rmse: 0.0954818\n",
      "[1675]\ttraining's rmse: 0.0918834\tvalid_1's rmse: 0.0954794\n",
      "[1700]\ttraining's rmse: 0.0918763\tvalid_1's rmse: 0.0954779\n",
      "[1725]\ttraining's rmse: 0.09187\tvalid_1's rmse: 0.0954761\n",
      "[1750]\ttraining's rmse: 0.0918621\tvalid_1's rmse: 0.0954737\n",
      "[1775]\ttraining's rmse: 0.0918569\tvalid_1's rmse: 0.0954731\n",
      "[1800]\ttraining's rmse: 0.09185\tvalid_1's rmse: 0.0954721\n",
      "[1825]\ttraining's rmse: 0.091845\tvalid_1's rmse: 0.0954708\n",
      "[1850]\ttraining's rmse: 0.0918408\tvalid_1's rmse: 0.0954695\n",
      "[1875]\ttraining's rmse: 0.0918358\tvalid_1's rmse: 0.0954674\n",
      "[1900]\ttraining's rmse: 0.0918314\tvalid_1's rmse: 0.0954662\n",
      "[1925]\ttraining's rmse: 0.0918283\tvalid_1's rmse: 0.0954653\n",
      "[1950]\ttraining's rmse: 0.0918236\tvalid_1's rmse: 0.0954636\n",
      "[1975]\ttraining's rmse: 0.0918187\tvalid_1's rmse: 0.0954626\n",
      "[2000]\ttraining's rmse: 0.0918138\tvalid_1's rmse: 0.0954625\n",
      "[2025]\ttraining's rmse: 0.0918088\tvalid_1's rmse: 0.0954611\n",
      "[2050]\ttraining's rmse: 0.0918061\tvalid_1's rmse: 0.0954603\n",
      "[2075]\ttraining's rmse: 0.0918024\tvalid_1's rmse: 0.0954592\n",
      "[2100]\ttraining's rmse: 0.0917965\tvalid_1's rmse: 0.0954579\n",
      "[2125]\ttraining's rmse: 0.0917944\tvalid_1's rmse: 0.0954576\n",
      "[2150]\ttraining's rmse: 0.0917912\tvalid_1's rmse: 0.0954567\n",
      "[2175]\ttraining's rmse: 0.0917879\tvalid_1's rmse: 0.0954558\n",
      "[2200]\ttraining's rmse: 0.0917852\tvalid_1's rmse: 0.0954546\n",
      "[2225]\ttraining's rmse: 0.0917818\tvalid_1's rmse: 0.0954534\n",
      "[2250]\ttraining's rmse: 0.0917792\tvalid_1's rmse: 0.0954531\n",
      "[2275]\ttraining's rmse: 0.0917756\tvalid_1's rmse: 0.0954515\n",
      "[2300]\ttraining's rmse: 0.091772\tvalid_1's rmse: 0.095451\n",
      "[2325]\ttraining's rmse: 0.0917688\tvalid_1's rmse: 0.0954485\n",
      "[2350]\ttraining's rmse: 0.0917663\tvalid_1's rmse: 0.0954484\n",
      "[2375]\ttraining's rmse: 0.0917639\tvalid_1's rmse: 0.0954475\n",
      "[2400]\ttraining's rmse: 0.09176\tvalid_1's rmse: 0.0954467\n",
      "[2425]\ttraining's rmse: 0.091758\tvalid_1's rmse: 0.0954464\n",
      "[2450]\ttraining's rmse: 0.0917559\tvalid_1's rmse: 0.0954449\n",
      "[2475]\ttraining's rmse: 0.0917536\tvalid_1's rmse: 0.0954454\n",
      "[2500]\ttraining's rmse: 0.0917508\tvalid_1's rmse: 0.0954446\n",
      "[2525]\ttraining's rmse: 0.0917491\tvalid_1's rmse: 0.0954433\n",
      "[2550]\ttraining's rmse: 0.0917481\tvalid_1's rmse: 0.0954434\n",
      "[2575]\ttraining's rmse: 0.0917451\tvalid_1's rmse: 0.0954426\n",
      "[2600]\ttraining's rmse: 0.0917426\tvalid_1's rmse: 0.0954409\n",
      "[2625]\ttraining's rmse: 0.0917398\tvalid_1's rmse: 0.0954405\n",
      "[2650]\ttraining's rmse: 0.0917382\tvalid_1's rmse: 0.0954404\n",
      "[2675]\ttraining's rmse: 0.0917357\tvalid_1's rmse: 0.0954393\n",
      "[2700]\ttraining's rmse: 0.0917336\tvalid_1's rmse: 0.0954397\n",
      "Early stopping, best iteration is:\n",
      "[2661]\ttraining's rmse: 0.0917371\tvalid_1's rmse: 0.0954392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/home/ningzesun/.local/lib/python3.6/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[25]\ttraining's rmse: 0.0949834\tvalid_1's rmse: 0.0957003\n",
      "[50]\ttraining's rmse: 0.0948029\tvalid_1's rmse: 0.0956181\n",
      "[75]\ttraining's rmse: 0.0946305\tvalid_1's rmse: 0.0955455\n",
      "[100]\ttraining's rmse: 0.0944698\tvalid_1's rmse: 0.0954783\n",
      "[125]\ttraining's rmse: 0.0943202\tvalid_1's rmse: 0.0954178\n",
      "[150]\ttraining's rmse: 0.0941774\tvalid_1's rmse: 0.0953629\n",
      "[175]\ttraining's rmse: 0.0940545\tvalid_1's rmse: 0.0953155\n",
      "[200]\ttraining's rmse: 0.0939306\tvalid_1's rmse: 0.0952712\n",
      "[225]\ttraining's rmse: 0.0938143\tvalid_1's rmse: 0.0952331\n",
      "[250]\ttraining's rmse: 0.0937118\tvalid_1's rmse: 0.0951968\n",
      "[275]\ttraining's rmse: 0.0936126\tvalid_1's rmse: 0.0951632\n",
      "[300]\ttraining's rmse: 0.093522\tvalid_1's rmse: 0.0951326\n",
      "[325]\ttraining's rmse: 0.0934351\tvalid_1's rmse: 0.0951053\n",
      "[350]\ttraining's rmse: 0.0933423\tvalid_1's rmse: 0.09508\n",
      "[375]\ttraining's rmse: 0.0932652\tvalid_1's rmse: 0.0950588\n",
      "[400]\ttraining's rmse: 0.093194\tvalid_1's rmse: 0.0950399\n",
      "[425]\ttraining's rmse: 0.0931282\tvalid_1's rmse: 0.0950203\n",
      "[450]\ttraining's rmse: 0.0930661\tvalid_1's rmse: 0.0950033\n",
      "[475]\ttraining's rmse: 0.093005\tvalid_1's rmse: 0.0949876\n",
      "[500]\ttraining's rmse: 0.0929519\tvalid_1's rmse: 0.0949737\n",
      "[525]\ttraining's rmse: 0.0928934\tvalid_1's rmse: 0.0949603\n",
      "[550]\ttraining's rmse: 0.0928341\tvalid_1's rmse: 0.0949492\n",
      "[575]\ttraining's rmse: 0.0927825\tvalid_1's rmse: 0.0949393\n",
      "[600]\ttraining's rmse: 0.0927292\tvalid_1's rmse: 0.094931\n",
      "[625]\ttraining's rmse: 0.0926883\tvalid_1's rmse: 0.0949229\n",
      "[650]\ttraining's rmse: 0.09264\tvalid_1's rmse: 0.0949138\n",
      "[675]\ttraining's rmse: 0.0925916\tvalid_1's rmse: 0.0949058\n",
      "[700]\ttraining's rmse: 0.0925472\tvalid_1's rmse: 0.0948994\n",
      "[725]\ttraining's rmse: 0.0925106\tvalid_1's rmse: 0.094895\n",
      "[750]\ttraining's rmse: 0.0924747\tvalid_1's rmse: 0.0948893\n",
      "[775]\ttraining's rmse: 0.0924445\tvalid_1's rmse: 0.0948836\n",
      "[800]\ttraining's rmse: 0.092408\tvalid_1's rmse: 0.09488\n",
      "[825]\ttraining's rmse: 0.0923779\tvalid_1's rmse: 0.0948761\n",
      "[850]\ttraining's rmse: 0.0923484\tvalid_1's rmse: 0.0948737\n",
      "[875]\ttraining's rmse: 0.0923214\tvalid_1's rmse: 0.0948705\n",
      "[900]\ttraining's rmse: 0.092289\tvalid_1's rmse: 0.0948682\n",
      "[925]\ttraining's rmse: 0.0922635\tvalid_1's rmse: 0.0948663\n",
      "[950]\ttraining's rmse: 0.092237\tvalid_1's rmse: 0.0948626\n",
      "[975]\ttraining's rmse: 0.0922113\tvalid_1's rmse: 0.0948605\n",
      "[1000]\ttraining's rmse: 0.0921924\tvalid_1's rmse: 0.0948586\n",
      "[1025]\ttraining's rmse: 0.0921636\tvalid_1's rmse: 0.0948563\n",
      "[1050]\ttraining's rmse: 0.0921434\tvalid_1's rmse: 0.0948556\n",
      "[1075]\ttraining's rmse: 0.0921228\tvalid_1's rmse: 0.0948543\n",
      "[1100]\ttraining's rmse: 0.0921038\tvalid_1's rmse: 0.0948518\n",
      "[1125]\ttraining's rmse: 0.0920838\tvalid_1's rmse: 0.0948511\n",
      "[1150]\ttraining's rmse: 0.092065\tvalid_1's rmse: 0.0948508\n",
      "[1175]\ttraining's rmse: 0.0920471\tvalid_1's rmse: 0.0948511\n",
      "[1200]\ttraining's rmse: 0.0920334\tvalid_1's rmse: 0.0948502\n",
      "[1225]\ttraining's rmse: 0.0920162\tvalid_1's rmse: 0.0948498\n",
      "[1250]\ttraining's rmse: 0.0920027\tvalid_1's rmse: 0.0948488\n",
      "[1275]\ttraining's rmse: 0.0919874\tvalid_1's rmse: 0.0948482\n",
      "[1300]\ttraining's rmse: 0.0919742\tvalid_1's rmse: 0.0948481\n",
      "[1325]\ttraining's rmse: 0.0919605\tvalid_1's rmse: 0.0948481\n",
      "[1350]\ttraining's rmse: 0.0919508\tvalid_1's rmse: 0.0948474\n",
      "[1375]\ttraining's rmse: 0.0919403\tvalid_1's rmse: 0.0948467\n",
      "[1400]\ttraining's rmse: 0.0919289\tvalid_1's rmse: 0.094846\n",
      "[1425]\ttraining's rmse: 0.0919193\tvalid_1's rmse: 0.0948457\n",
      "[1450]\ttraining's rmse: 0.0919084\tvalid_1's rmse: 0.0948459\n",
      "[1475]\ttraining's rmse: 0.091899\tvalid_1's rmse: 0.0948465\n",
      "Early stopping, best iteration is:\n",
      "[1435]\ttraining's rmse: 0.0919146\tvalid_1's rmse: 0.0948456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/home/ningzesun/.local/lib/python3.6/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[25]\ttraining's rmse: 0.0960251\tvalid_1's rmse: 0.0935749\n",
      "[50]\ttraining's rmse: 0.0958409\tvalid_1's rmse: 0.0935081\n",
      "[75]\ttraining's rmse: 0.0956641\tvalid_1's rmse: 0.0934471\n",
      "[100]\ttraining's rmse: 0.0955022\tvalid_1's rmse: 0.0933901\n",
      "[125]\ttraining's rmse: 0.0953478\tvalid_1's rmse: 0.0933387\n",
      "[150]\ttraining's rmse: 0.0952022\tvalid_1's rmse: 0.0932924\n",
      "[175]\ttraining's rmse: 0.0950772\tvalid_1's rmse: 0.0932545\n",
      "[200]\ttraining's rmse: 0.0949471\tvalid_1's rmse: 0.0932178\n",
      "[225]\ttraining's rmse: 0.0948271\tvalid_1's rmse: 0.0931842\n",
      "[250]\ttraining's rmse: 0.0947226\tvalid_1's rmse: 0.0931577\n",
      "[275]\ttraining's rmse: 0.09462\tvalid_1's rmse: 0.0931292\n",
      "[300]\ttraining's rmse: 0.0945209\tvalid_1's rmse: 0.0931058\n",
      "[325]\ttraining's rmse: 0.0944296\tvalid_1's rmse: 0.0930843\n",
      "[350]\ttraining's rmse: 0.0943317\tvalid_1's rmse: 0.0930634\n",
      "[375]\ttraining's rmse: 0.0942483\tvalid_1's rmse: 0.093047\n",
      "[400]\ttraining's rmse: 0.094171\tvalid_1's rmse: 0.0930303\n",
      "[425]\ttraining's rmse: 0.0940964\tvalid_1's rmse: 0.0930147\n",
      "[450]\ttraining's rmse: 0.0940312\tvalid_1's rmse: 0.0930117\n",
      "[475]\ttraining's rmse: 0.0939658\tvalid_1's rmse: 0.0929998\n",
      "[500]\ttraining's rmse: 0.0939071\tvalid_1's rmse: 0.0929955\n",
      "[525]\ttraining's rmse: 0.0938427\tvalid_1's rmse: 0.0929827\n",
      "[550]\ttraining's rmse: 0.0937784\tvalid_1's rmse: 0.0929723\n",
      "[575]\ttraining's rmse: 0.0937233\tvalid_1's rmse: 0.0929646\n",
      "[600]\ttraining's rmse: 0.0936639\tvalid_1's rmse: 0.0929568\n",
      "[625]\ttraining's rmse: 0.0936181\tvalid_1's rmse: 0.0929611\n",
      "[650]\ttraining's rmse: 0.0935682\tvalid_1's rmse: 0.0929608\n",
      "Early stopping, best iteration is:\n",
      "[612]\ttraining's rmse: 0.0936402\tvalid_1's rmse: 0.0929528\n"
     ]
    }
   ],
   "source": [
    "score = 0\n",
    "kf = KFold(n_splits=3)\n",
    "models = []\n",
    "\n",
    "params = {\"objective\": \"regression\", \"boosting\": \"gbdt\", \"num_leaves\": 700, \n",
    "              \"learning_rate\": 0.002646, 'bagging_fraction': 0.8296, \"reg_lambda\": 0.0963, \n",
    "              'reg_alpha':0.1066, \"metric\": \"rmse\", 'max_depth': -1, 'min_child_weight': 27,\n",
    "              'verbose': -1, 'min_split_gain':0.08097 , 'subsample_freq':1, 'sub_feature':  0.5809}\n",
    "for train_index,test_index in kf.split(features):\n",
    "    train_features = features.loc[train_index]\n",
    "    train_target = target.loc[train_index]\n",
    "    test_features = features.loc[test_index]\n",
    "    test_target = target.loc[test_index]\n",
    "    d_training = lgb.Dataset(train_features, label=train_target, \n",
    "                             categorical_feature=categorical_features, free_raw_data=False)\n",
    "    d_test = lgb.Dataset(test_features, label=test_target, \n",
    "                         categorical_feature=categorical_features, free_raw_data=False)\n",
    "    model = lgb.train(params, train_set=d_training, num_boost_round=3000, \n",
    "                      valid_sets=[d_training,d_test], verbose_eval=25, early_stopping_rounds=50)\n",
    "    y_pred_valid = model.predict(test_features)\n",
    "    score += np.sqrt(mean_squared_error(test_target, y_pred_valid)) / 3\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1087.64 MB\n",
      "Memory usage after optimization is: 428.55 MB\n",
      "Decreased by 60.6%\n",
      "Memory usage of dataframe is 1227.28 MB\n",
      "Memory usage after optimization is: 483.58 MB\n",
      "Decreased by 60.6%\n"
     ]
    }
   ],
   "source": [
    "del df_merge\n",
    "gc.collect()\n",
    "df_sub_2016 = reduce_mem_usage(pd.read_csv('./output/final_sub_2016.csv')).drop_duplicates('parcelid')\n",
    "df_sub_2016['year'] = 0\n",
    "results = {}\n",
    "for month in [10,11,12]:\n",
    "    df_sub_2016['month'] = month\n",
    "    for i in models:\n",
    "        if  month not in results.keys():\n",
    "            results[month] = i.predict(df_sub_2016[list(features.columns)], \n",
    "                                       num_iteration=i.best_iteration) / len(models)\n",
    "        else:\n",
    "            results[month] += i.predict(df_sub_2016[list(features.columns)], \n",
    "                                        num_iteration=i.best_iteration) / len(models)\n",
    "for i in results.keys():\n",
    "    df_sub_2016[str(i)] = results[i]\n",
    "df_sub = pd.read_csv('./Resources/sample_submission.csv')\n",
    "df_sub = df_sub.rename(columns = {'ParcelId': 'parcelid'})\n",
    "df_sub = pd.merge(df_sub[['parcelid']], df_sub_2016[['parcelid', '10', '11', '12']].drop_duplicates('parcelid'),\n",
    "                  how = 'left', on = 'parcelid')\n",
    "df_sub = df_sub.rename(columns = {'10': '201610', '11': '201611', '12': '201612'}).drop_duplicates('parcelid')\n",
    "del df_sub_2016\n",
    "gc.collect()\n",
    "df_sub_2017 = reduce_mem_usage(pd.read_csv('./output/final_sub_2017.csv'))\n",
    "df_sub_2017['year'] = 1\n",
    "results = {}\n",
    "for month in [10,11,12]:\n",
    "    df_sub_2017['month'] = month\n",
    "    for i in models:\n",
    "        if  month not in results.keys():\n",
    "            results[month] = i.predict(df_sub_2017[list(features.columns)], \n",
    "                                       num_iteration=i.best_iteration) / len(models)\n",
    "        else:\n",
    "            results[month] += i.predict(df_sub_2017[list(features.columns)], \n",
    "                                        num_iteration=i.best_iteration) / len(models)\n",
    "for i in results.keys():\n",
    "    df_sub_2017[str(i)] = results[i]\n",
    "df_sub = pd.merge(df_sub[['parcelid', '201610', '201611', '201612']], \n",
    "                  df_sub_2017[['parcelid', '10', '11', '12']].drop_duplicates('parcelid'), \n",
    "                  how = 'left', on = 'parcelid')\n",
    "df_sub = df_sub.rename(columns = {'10': '201710', '11': '201711', '12': '201712'})\n",
    "del df_sub_2017\n",
    "gc.collect()\n",
    "df_sub.to_csv('./output/sample_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parcelid</th>\n",
       "      <th>201610</th>\n",
       "      <th>201611</th>\n",
       "      <th>201612</th>\n",
       "      <th>201710</th>\n",
       "      <th>201711</th>\n",
       "      <th>201712</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10754147</td>\n",
       "      <td>-0.025716</td>\n",
       "      <td>-0.025634</td>\n",
       "      <td>-0.025575</td>\n",
       "      <td>-0.021063</td>\n",
       "      <td>-0.020982</td>\n",
       "      <td>-0.020923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10759547</td>\n",
       "      <td>-0.004304</td>\n",
       "      <td>-0.004321</td>\n",
       "      <td>-0.004262</td>\n",
       "      <td>0.000949</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>0.000991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10843547</td>\n",
       "      <td>0.002021</td>\n",
       "      <td>0.002021</td>\n",
       "      <td>0.002021</td>\n",
       "      <td>0.003166</td>\n",
       "      <td>0.003158</td>\n",
       "      <td>0.003310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10859147</td>\n",
       "      <td>0.019901</td>\n",
       "      <td>0.019901</td>\n",
       "      <td>0.019930</td>\n",
       "      <td>0.018740</td>\n",
       "      <td>0.018740</td>\n",
       "      <td>0.018768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10879947</td>\n",
       "      <td>0.008085</td>\n",
       "      <td>0.008152</td>\n",
       "      <td>0.008233</td>\n",
       "      <td>0.008025</td>\n",
       "      <td>0.008092</td>\n",
       "      <td>0.008173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2985212</th>\n",
       "      <td>168176230</td>\n",
       "      <td>0.003283</td>\n",
       "      <td>0.003245</td>\n",
       "      <td>0.003283</td>\n",
       "      <td>0.027966</td>\n",
       "      <td>0.028009</td>\n",
       "      <td>0.028026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2985213</th>\n",
       "      <td>14273630</td>\n",
       "      <td>0.003313</td>\n",
       "      <td>0.003275</td>\n",
       "      <td>0.003313</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>0.020851</td>\n",
       "      <td>0.020861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2985214</th>\n",
       "      <td>168040630</td>\n",
       "      <td>0.003283</td>\n",
       "      <td>0.003245</td>\n",
       "      <td>0.003283</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.009533</td>\n",
       "      <td>0.009555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2985215</th>\n",
       "      <td>168040830</td>\n",
       "      <td>0.003191</td>\n",
       "      <td>0.003153</td>\n",
       "      <td>0.003191</td>\n",
       "      <td>0.111149</td>\n",
       "      <td>0.111058</td>\n",
       "      <td>0.111189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2985216</th>\n",
       "      <td>168040430</td>\n",
       "      <td>0.003191</td>\n",
       "      <td>0.003153</td>\n",
       "      <td>0.003191</td>\n",
       "      <td>0.011505</td>\n",
       "      <td>0.011505</td>\n",
       "      <td>0.011527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2985217 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          parcelid    201610    201611    201612    201710    201711    201712\n",
       "0         10754147 -0.025716 -0.025634 -0.025575 -0.021063 -0.020982 -0.020923\n",
       "1         10759547 -0.004304 -0.004321 -0.004262  0.000949  0.000932  0.000991\n",
       "2         10843547  0.002021  0.002021  0.002021  0.003166  0.003158  0.003310\n",
       "3         10859147  0.019901  0.019901  0.019930  0.018740  0.018740  0.018768\n",
       "4         10879947  0.008085  0.008152  0.008233  0.008025  0.008092  0.008173\n",
       "...            ...       ...       ...       ...       ...       ...       ...\n",
       "2985212  168176230  0.003283  0.003245  0.003283  0.027966  0.028009  0.028026\n",
       "2985213   14273630  0.003313  0.003275  0.003313  0.020822  0.020851  0.020861\n",
       "2985214  168040630  0.003283  0.003245  0.003283  0.009524  0.009533  0.009555\n",
       "2985215  168040830  0.003191  0.003153  0.003191  0.111149  0.111058  0.111189\n",
       "2985216  168040430  0.003191  0.003153  0.003191  0.011505  0.011505  0.011527\n",
       "\n",
       "[2985217 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
