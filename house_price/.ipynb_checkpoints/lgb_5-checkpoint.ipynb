{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "\n",
    "def reduce_mem_usage(df, use_float16=False):\n",
    "    \"\"\"\n",
    "    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    \n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n",
    "            continue\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
    "    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 65.85 MB\n",
      "Memory usage after optimization is: 26.09 MB\n",
      "Decreased by 60.4%\n"
     ]
    }
   ],
   "source": [
    "df_merge = reduce_mem_usage(pd.read_csv('./output/final_merge.csv'))\n",
    "categorical_features = ['airconditioningtypeid', 'hashottuborspa', 'heatingorsystemtypeid', \n",
    "                       'pooltypeid2', 'propertylandusetypeid', 'fips', 'regionidcounty', \n",
    "                       'buildingqualitytypeid_fill', 'regionidcity_fill', 'year', \n",
    "                       'regionidneighborhood_fill', 'taxdelinquencyflag']\n",
    "df_drop = df_merge.drop_duplicates(subset = ['parcelid', 'logerror'])\n",
    "df_drop = df_drop[ df_drop.logerror > -0.4 ]\n",
    "df_drop = df_drop[ df_drop.logerror < 0.419 ]\n",
    "df_drop = df_drop.reset_index(drop = True)\n",
    "target = df_drop.logerror\n",
    "features = df_drop.drop(['logerror'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop.to_csv('./output/outlier_remove.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/home/ningzesun/.local/lib/python3.6/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[25]\ttraining's rmse: 0.0824427\tvalid_1's rmse: 0.0841988\n",
      "[50]\ttraining's rmse: 0.0823596\tvalid_1's rmse: 0.0841425\n",
      "[75]\ttraining's rmse: 0.082281\tvalid_1's rmse: 0.0840867\n",
      "[100]\ttraining's rmse: 0.0822077\tvalid_1's rmse: 0.0840381\n",
      "[125]\ttraining's rmse: 0.0821311\tvalid_1's rmse: 0.0839897\n",
      "[150]\ttraining's rmse: 0.082059\tvalid_1's rmse: 0.083944\n",
      "[175]\ttraining's rmse: 0.0820008\tvalid_1's rmse: 0.0839048\n",
      "[200]\ttraining's rmse: 0.0819378\tvalid_1's rmse: 0.0838658\n",
      "[225]\ttraining's rmse: 0.0818748\tvalid_1's rmse: 0.08383\n",
      "[250]\ttraining's rmse: 0.0818242\tvalid_1's rmse: 0.0837971\n",
      "[275]\ttraining's rmse: 0.0817757\tvalid_1's rmse: 0.0837682\n",
      "[300]\ttraining's rmse: 0.0817254\tvalid_1's rmse: 0.0837367\n",
      "[325]\ttraining's rmse: 0.0816771\tvalid_1's rmse: 0.0837092\n",
      "[350]\ttraining's rmse: 0.0816309\tvalid_1's rmse: 0.0836837\n",
      "[375]\ttraining's rmse: 0.0815944\tvalid_1's rmse: 0.0836615\n",
      "[400]\ttraining's rmse: 0.0815523\tvalid_1's rmse: 0.0836401\n",
      "[425]\ttraining's rmse: 0.0815159\tvalid_1's rmse: 0.0836192\n",
      "[450]\ttraining's rmse: 0.0814812\tvalid_1's rmse: 0.0835975\n",
      "[475]\ttraining's rmse: 0.0814528\tvalid_1's rmse: 0.0835775\n",
      "[500]\ttraining's rmse: 0.0814271\tvalid_1's rmse: 0.0835616\n",
      "[525]\ttraining's rmse: 0.0813925\tvalid_1's rmse: 0.0835454\n",
      "[550]\ttraining's rmse: 0.0813577\tvalid_1's rmse: 0.0835288\n",
      "[575]\ttraining's rmse: 0.0813294\tvalid_1's rmse: 0.0835158\n",
      "[600]\ttraining's rmse: 0.0813008\tvalid_1's rmse: 0.0835022\n",
      "[625]\ttraining's rmse: 0.0812787\tvalid_1's rmse: 0.0834883\n",
      "[650]\ttraining's rmse: 0.0812499\tvalid_1's rmse: 0.0834758\n",
      "[675]\ttraining's rmse: 0.0812212\tvalid_1's rmse: 0.083463\n",
      "[700]\ttraining's rmse: 0.0811971\tvalid_1's rmse: 0.0834513\n",
      "[725]\ttraining's rmse: 0.0811751\tvalid_1's rmse: 0.0834412\n",
      "[750]\ttraining's rmse: 0.0811521\tvalid_1's rmse: 0.0834312\n",
      "[775]\ttraining's rmse: 0.0811336\tvalid_1's rmse: 0.0834205\n",
      "[800]\ttraining's rmse: 0.0811105\tvalid_1's rmse: 0.083411\n",
      "[825]\ttraining's rmse: 0.0810902\tvalid_1's rmse: 0.0834011\n",
      "[850]\ttraining's rmse: 0.0810695\tvalid_1's rmse: 0.0833934\n",
      "[875]\ttraining's rmse: 0.0810514\tvalid_1's rmse: 0.0833858\n",
      "[900]\ttraining's rmse: 0.0810301\tvalid_1's rmse: 0.0833774\n",
      "[925]\ttraining's rmse: 0.0810113\tvalid_1's rmse: 0.083368\n",
      "[950]\ttraining's rmse: 0.0809952\tvalid_1's rmse: 0.0833602\n",
      "[975]\ttraining's rmse: 0.0809803\tvalid_1's rmse: 0.0833529\n",
      "[1000]\ttraining's rmse: 0.0809653\tvalid_1's rmse: 0.083347\n",
      "[1025]\ttraining's rmse: 0.080949\tvalid_1's rmse: 0.0833398\n",
      "[1050]\ttraining's rmse: 0.0809331\tvalid_1's rmse: 0.0833326\n",
      "[1075]\ttraining's rmse: 0.0809186\tvalid_1's rmse: 0.0833274\n",
      "[1100]\ttraining's rmse: 0.0809055\tvalid_1's rmse: 0.0833223\n",
      "[1125]\ttraining's rmse: 0.0808935\tvalid_1's rmse: 0.0833155\n",
      "[1150]\ttraining's rmse: 0.0808799\tvalid_1's rmse: 0.0833103\n",
      "[1175]\ttraining's rmse: 0.0808692\tvalid_1's rmse: 0.0833061\n",
      "[1200]\ttraining's rmse: 0.0808586\tvalid_1's rmse: 0.0833013\n",
      "[1225]\ttraining's rmse: 0.0808475\tvalid_1's rmse: 0.0832959\n",
      "[1250]\ttraining's rmse: 0.0808353\tvalid_1's rmse: 0.0832919\n",
      "[1275]\ttraining's rmse: 0.0808222\tvalid_1's rmse: 0.0832869\n",
      "[1300]\ttraining's rmse: 0.0808136\tvalid_1's rmse: 0.0832814\n",
      "[1325]\ttraining's rmse: 0.0808024\tvalid_1's rmse: 0.0832779\n",
      "[1350]\ttraining's rmse: 0.0807919\tvalid_1's rmse: 0.0832746\n",
      "[1375]\ttraining's rmse: 0.0807817\tvalid_1's rmse: 0.0832709\n",
      "[1400]\ttraining's rmse: 0.0807736\tvalid_1's rmse: 0.0832656\n",
      "[1425]\ttraining's rmse: 0.0807646\tvalid_1's rmse: 0.0832639\n",
      "[1450]\ttraining's rmse: 0.0807572\tvalid_1's rmse: 0.0832599\n",
      "[1475]\ttraining's rmse: 0.0807489\tvalid_1's rmse: 0.0832563\n",
      "[1500]\ttraining's rmse: 0.0807405\tvalid_1's rmse: 0.0832525\n",
      "[1525]\ttraining's rmse: 0.0807335\tvalid_1's rmse: 0.0832507\n",
      "[1550]\ttraining's rmse: 0.0807252\tvalid_1's rmse: 0.0832478\n",
      "[1575]\ttraining's rmse: 0.0807182\tvalid_1's rmse: 0.0832457\n",
      "[1600]\ttraining's rmse: 0.0807109\tvalid_1's rmse: 0.0832434\n",
      "[1625]\ttraining's rmse: 0.0807046\tvalid_1's rmse: 0.0832406\n",
      "[1650]\ttraining's rmse: 0.0806991\tvalid_1's rmse: 0.0832392\n",
      "[1675]\ttraining's rmse: 0.0806938\tvalid_1's rmse: 0.0832372\n",
      "[1700]\ttraining's rmse: 0.0806897\tvalid_1's rmse: 0.0832347\n",
      "[1725]\ttraining's rmse: 0.0806854\tvalid_1's rmse: 0.0832324\n",
      "[1750]\ttraining's rmse: 0.0806804\tvalid_1's rmse: 0.0832281\n",
      "[1775]\ttraining's rmse: 0.0806753\tvalid_1's rmse: 0.0832262\n",
      "[1800]\ttraining's rmse: 0.0806709\tvalid_1's rmse: 0.0832251\n",
      "[1825]\ttraining's rmse: 0.080666\tvalid_1's rmse: 0.0832228\n",
      "[1850]\ttraining's rmse: 0.0806615\tvalid_1's rmse: 0.0832204\n",
      "[1875]\ttraining's rmse: 0.0806585\tvalid_1's rmse: 0.0832192\n",
      "[1900]\ttraining's rmse: 0.0806549\tvalid_1's rmse: 0.0832178\n",
      "[1925]\ttraining's rmse: 0.0806524\tvalid_1's rmse: 0.083217\n",
      "[1950]\ttraining's rmse: 0.0806501\tvalid_1's rmse: 0.0832158\n",
      "[1975]\ttraining's rmse: 0.0806468\tvalid_1's rmse: 0.0832148\n",
      "[2000]\ttraining's rmse: 0.0806445\tvalid_1's rmse: 0.083214\n",
      "[2025]\ttraining's rmse: 0.0806424\tvalid_1's rmse: 0.0832127\n",
      "[2050]\ttraining's rmse: 0.0806386\tvalid_1's rmse: 0.0832109\n",
      "[2075]\ttraining's rmse: 0.0806359\tvalid_1's rmse: 0.0832097\n",
      "[2100]\ttraining's rmse: 0.0806324\tvalid_1's rmse: 0.083208\n",
      "[2125]\ttraining's rmse: 0.0806305\tvalid_1's rmse: 0.0832069\n",
      "[2150]\ttraining's rmse: 0.0806274\tvalid_1's rmse: 0.0832064\n",
      "[2175]\ttraining's rmse: 0.0806245\tvalid_1's rmse: 0.0832056\n",
      "[2200]\ttraining's rmse: 0.0806221\tvalid_1's rmse: 0.0832052\n",
      "[2225]\ttraining's rmse: 0.0806194\tvalid_1's rmse: 0.0832033\n",
      "[2250]\ttraining's rmse: 0.080617\tvalid_1's rmse: 0.0832019\n",
      "[2275]\ttraining's rmse: 0.0806145\tvalid_1's rmse: 0.0832003\n",
      "[2300]\ttraining's rmse: 0.0806121\tvalid_1's rmse: 0.0831996\n",
      "[2325]\ttraining's rmse: 0.0806094\tvalid_1's rmse: 0.083198\n",
      "[2350]\ttraining's rmse: 0.0806075\tvalid_1's rmse: 0.0831971\n",
      "[2375]\ttraining's rmse: 0.0806053\tvalid_1's rmse: 0.0831969\n",
      "[2400]\ttraining's rmse: 0.0806015\tvalid_1's rmse: 0.0831956\n",
      "[2425]\ttraining's rmse: 0.0806002\tvalid_1's rmse: 0.0831956\n",
      "[2450]\ttraining's rmse: 0.0805988\tvalid_1's rmse: 0.083195\n",
      "[2475]\ttraining's rmse: 0.0805964\tvalid_1's rmse: 0.0831951\n",
      "[2500]\ttraining's rmse: 0.0805949\tvalid_1's rmse: 0.0831944\n",
      "[2525]\ttraining's rmse: 0.080593\tvalid_1's rmse: 0.083194\n",
      "[2550]\ttraining's rmse: 0.0805906\tvalid_1's rmse: 0.083194\n",
      "[2575]\ttraining's rmse: 0.0805888\tvalid_1's rmse: 0.0831935\n",
      "[2600]\ttraining's rmse: 0.080587\tvalid_1's rmse: 0.0831923\n",
      "[2625]\ttraining's rmse: 0.0805851\tvalid_1's rmse: 0.0831911\n",
      "[2650]\ttraining's rmse: 0.0805839\tvalid_1's rmse: 0.0831909\n",
      "[2675]\ttraining's rmse: 0.0805817\tvalid_1's rmse: 0.0831896\n",
      "[2700]\ttraining's rmse: 0.0805805\tvalid_1's rmse: 0.0831893\n",
      "[2725]\ttraining's rmse: 0.0805784\tvalid_1's rmse: 0.0831886\n",
      "[2750]\ttraining's rmse: 0.0805763\tvalid_1's rmse: 0.0831874\n",
      "[2775]\ttraining's rmse: 0.0805749\tvalid_1's rmse: 0.0831875\n",
      "[2800]\ttraining's rmse: 0.0805735\tvalid_1's rmse: 0.0831876\n",
      "[2825]\ttraining's rmse: 0.0805709\tvalid_1's rmse: 0.0831865\n",
      "[2850]\ttraining's rmse: 0.0805699\tvalid_1's rmse: 0.083186\n",
      "[2875]\ttraining's rmse: 0.0805686\tvalid_1's rmse: 0.0831859\n",
      "[2900]\ttraining's rmse: 0.0805676\tvalid_1's rmse: 0.0831859\n",
      "[2925]\ttraining's rmse: 0.080566\tvalid_1's rmse: 0.0831855\n",
      "[2950]\ttraining's rmse: 0.0805652\tvalid_1's rmse: 0.0831853\n",
      "[2975]\ttraining's rmse: 0.0805639\tvalid_1's rmse: 0.0831844\n",
      "[3000]\ttraining's rmse: 0.0805616\tvalid_1's rmse: 0.0831838\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's rmse: 0.0805616\tvalid_1's rmse: 0.0831838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/home/ningzesun/.local/lib/python3.6/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[25]\ttraining's rmse: 0.0826771\tvalid_1's rmse: 0.0837032\n",
      "[50]\ttraining's rmse: 0.0825798\tvalid_1's rmse: 0.0836486\n",
      "[75]\ttraining's rmse: 0.0824844\tvalid_1's rmse: 0.083597\n",
      "[100]\ttraining's rmse: 0.0824013\tvalid_1's rmse: 0.0835519\n",
      "[125]\ttraining's rmse: 0.082312\tvalid_1's rmse: 0.0835045\n",
      "[150]\ttraining's rmse: 0.0822268\tvalid_1's rmse: 0.0834592\n",
      "[175]\ttraining's rmse: 0.0821574\tvalid_1's rmse: 0.0834229\n",
      "[200]\ttraining's rmse: 0.0820828\tvalid_1's rmse: 0.083387\n",
      "[225]\ttraining's rmse: 0.082013\tvalid_1's rmse: 0.0833531\n",
      "[250]\ttraining's rmse: 0.0819541\tvalid_1's rmse: 0.0833217\n",
      "[275]\ttraining's rmse: 0.0818989\tvalid_1's rmse: 0.0832946\n",
      "[300]\ttraining's rmse: 0.081842\tvalid_1's rmse: 0.0832672\n",
      "[325]\ttraining's rmse: 0.0817846\tvalid_1's rmse: 0.0832405\n",
      "[350]\ttraining's rmse: 0.0817319\tvalid_1's rmse: 0.0832166\n",
      "[375]\ttraining's rmse: 0.0816883\tvalid_1's rmse: 0.083196\n",
      "[400]\ttraining's rmse: 0.0816422\tvalid_1's rmse: 0.0831774\n",
      "[425]\ttraining's rmse: 0.0816007\tvalid_1's rmse: 0.0831587\n",
      "[450]\ttraining's rmse: 0.0815581\tvalid_1's rmse: 0.0831392\n",
      "[475]\ttraining's rmse: 0.0815231\tvalid_1's rmse: 0.0831229\n",
      "[500]\ttraining's rmse: 0.0814933\tvalid_1's rmse: 0.0831065\n",
      "[525]\ttraining's rmse: 0.0814562\tvalid_1's rmse: 0.083092\n",
      "[550]\ttraining's rmse: 0.0814186\tvalid_1's rmse: 0.0830768\n",
      "[575]\ttraining's rmse: 0.0813855\tvalid_1's rmse: 0.0830643\n",
      "[600]\ttraining's rmse: 0.0813538\tvalid_1's rmse: 0.0830524\n",
      "[625]\ttraining's rmse: 0.0813274\tvalid_1's rmse: 0.0830405\n",
      "[650]\ttraining's rmse: 0.081296\tvalid_1's rmse: 0.0830289\n",
      "[675]\ttraining's rmse: 0.0812633\tvalid_1's rmse: 0.0830175\n",
      "[700]\ttraining's rmse: 0.0812347\tvalid_1's rmse: 0.0830065\n",
      "[725]\ttraining's rmse: 0.0812093\tvalid_1's rmse: 0.0829974\n",
      "[750]\ttraining's rmse: 0.0811849\tvalid_1's rmse: 0.0829887\n",
      "[775]\ttraining's rmse: 0.0811625\tvalid_1's rmse: 0.0829799\n",
      "[800]\ttraining's rmse: 0.0811363\tvalid_1's rmse: 0.0829725\n",
      "[825]\ttraining's rmse: 0.0811135\tvalid_1's rmse: 0.0829654\n",
      "[850]\ttraining's rmse: 0.0810913\tvalid_1's rmse: 0.0829595\n",
      "[875]\ttraining's rmse: 0.0810699\tvalid_1's rmse: 0.0829527\n",
      "[900]\ttraining's rmse: 0.081047\tvalid_1's rmse: 0.0829463\n",
      "[925]\ttraining's rmse: 0.0810254\tvalid_1's rmse: 0.0829401\n",
      "[950]\ttraining's rmse: 0.0810073\tvalid_1's rmse: 0.082934\n",
      "[975]\ttraining's rmse: 0.0809909\tvalid_1's rmse: 0.0829286\n",
      "[1000]\ttraining's rmse: 0.0809729\tvalid_1's rmse: 0.0829227\n",
      "[1025]\ttraining's rmse: 0.080953\tvalid_1's rmse: 0.0829181\n",
      "[1050]\ttraining's rmse: 0.0809361\tvalid_1's rmse: 0.0829124\n",
      "[1075]\ttraining's rmse: 0.0809184\tvalid_1's rmse: 0.0829071\n",
      "[1100]\ttraining's rmse: 0.0809055\tvalid_1's rmse: 0.082904\n",
      "[1125]\ttraining's rmse: 0.0808904\tvalid_1's rmse: 0.0829002\n",
      "[1150]\ttraining's rmse: 0.0808762\tvalid_1's rmse: 0.0828966\n",
      "[1175]\ttraining's rmse: 0.0808634\tvalid_1's rmse: 0.0828933\n",
      "[1200]\ttraining's rmse: 0.0808513\tvalid_1's rmse: 0.0828901\n",
      "[1225]\ttraining's rmse: 0.0808375\tvalid_1's rmse: 0.0828872\n",
      "[1250]\ttraining's rmse: 0.0808235\tvalid_1's rmse: 0.0828842\n",
      "[1275]\ttraining's rmse: 0.0808087\tvalid_1's rmse: 0.0828811\n",
      "[1300]\ttraining's rmse: 0.080799\tvalid_1's rmse: 0.0828779\n",
      "[1325]\ttraining's rmse: 0.0807875\tvalid_1's rmse: 0.0828763\n",
      "[1350]\ttraining's rmse: 0.0807776\tvalid_1's rmse: 0.0828745\n",
      "[1375]\ttraining's rmse: 0.0807662\tvalid_1's rmse: 0.0828729\n",
      "[1400]\ttraining's rmse: 0.0807569\tvalid_1's rmse: 0.0828712\n",
      "[1425]\ttraining's rmse: 0.0807471\tvalid_1's rmse: 0.0828701\n",
      "[1450]\ttraining's rmse: 0.0807365\tvalid_1's rmse: 0.0828677\n",
      "[1475]\ttraining's rmse: 0.0807289\tvalid_1's rmse: 0.0828662\n",
      "[1500]\ttraining's rmse: 0.0807196\tvalid_1's rmse: 0.0828647\n",
      "[1525]\ttraining's rmse: 0.08071\tvalid_1's rmse: 0.0828626\n",
      "[1550]\ttraining's rmse: 0.0807018\tvalid_1's rmse: 0.082862\n",
      "[1575]\ttraining's rmse: 0.0806944\tvalid_1's rmse: 0.0828608\n",
      "[1600]\ttraining's rmse: 0.0806879\tvalid_1's rmse: 0.0828605\n",
      "[1625]\ttraining's rmse: 0.0806793\tvalid_1's rmse: 0.0828594\n",
      "[1650]\ttraining's rmse: 0.0806729\tvalid_1's rmse: 0.0828586\n",
      "[1675]\ttraining's rmse: 0.0806673\tvalid_1's rmse: 0.0828576\n",
      "[1700]\ttraining's rmse: 0.0806628\tvalid_1's rmse: 0.0828569\n",
      "[1725]\ttraining's rmse: 0.0806581\tvalid_1's rmse: 0.0828563\n",
      "[1750]\ttraining's rmse: 0.0806522\tvalid_1's rmse: 0.0828556\n",
      "[1775]\ttraining's rmse: 0.0806465\tvalid_1's rmse: 0.0828545\n",
      "[1800]\ttraining's rmse: 0.0806411\tvalid_1's rmse: 0.0828538\n",
      "[1825]\ttraining's rmse: 0.080635\tvalid_1's rmse: 0.0828529\n",
      "[1850]\ttraining's rmse: 0.0806307\tvalid_1's rmse: 0.0828517\n",
      "[1875]\ttraining's rmse: 0.0806254\tvalid_1's rmse: 0.0828505\n",
      "[1900]\ttraining's rmse: 0.0806206\tvalid_1's rmse: 0.0828499\n",
      "[1925]\ttraining's rmse: 0.0806178\tvalid_1's rmse: 0.0828499\n",
      "[1950]\ttraining's rmse: 0.080615\tvalid_1's rmse: 0.0828496\n",
      "[1975]\ttraining's rmse: 0.0806113\tvalid_1's rmse: 0.0828488\n",
      "[2000]\ttraining's rmse: 0.0806079\tvalid_1's rmse: 0.0828483\n",
      "[2025]\ttraining's rmse: 0.0806046\tvalid_1's rmse: 0.0828471\n",
      "[2050]\ttraining's rmse: 0.0806004\tvalid_1's rmse: 0.0828469\n",
      "[2075]\ttraining's rmse: 0.0805968\tvalid_1's rmse: 0.082846\n",
      "[2100]\ttraining's rmse: 0.0805925\tvalid_1's rmse: 0.0828454\n",
      "[2125]\ttraining's rmse: 0.0805898\tvalid_1's rmse: 0.0828453\n",
      "[2150]\ttraining's rmse: 0.0805851\tvalid_1's rmse: 0.0828449\n",
      "[2175]\ttraining's rmse: 0.0805824\tvalid_1's rmse: 0.0828446\n",
      "[2200]\ttraining's rmse: 0.0805788\tvalid_1's rmse: 0.0828442\n",
      "[2225]\ttraining's rmse: 0.0805766\tvalid_1's rmse: 0.0828441\n",
      "[2250]\ttraining's rmse: 0.0805745\tvalid_1's rmse: 0.0828439\n",
      "[2275]\ttraining's rmse: 0.0805703\tvalid_1's rmse: 0.0828433\n",
      "[2300]\ttraining's rmse: 0.0805672\tvalid_1's rmse: 0.0828424\n",
      "[2325]\ttraining's rmse: 0.0805648\tvalid_1's rmse: 0.0828421\n",
      "[2350]\ttraining's rmse: 0.0805625\tvalid_1's rmse: 0.0828422\n",
      "[2375]\ttraining's rmse: 0.0805606\tvalid_1's rmse: 0.0828423\n",
      "Early stopping, best iteration is:\n",
      "[2335]\ttraining's rmse: 0.0805642\tvalid_1's rmse: 0.082842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/home/ningzesun/.local/lib/python3.6/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[25]\ttraining's rmse: 0.0838518\tvalid_1's rmse: 0.0813017\n",
      "[50]\ttraining's rmse: 0.0837527\tvalid_1's rmse: 0.0812532\n",
      "[75]\ttraining's rmse: 0.0836533\tvalid_1's rmse: 0.0812083\n",
      "[100]\ttraining's rmse: 0.083566\tvalid_1's rmse: 0.0811682\n",
      "[125]\ttraining's rmse: 0.0834712\tvalid_1's rmse: 0.0811284\n",
      "[150]\ttraining's rmse: 0.0833804\tvalid_1's rmse: 0.0810926\n",
      "[175]\ttraining's rmse: 0.0833072\tvalid_1's rmse: 0.0810605\n",
      "[200]\ttraining's rmse: 0.0832299\tvalid_1's rmse: 0.0810308\n",
      "[225]\ttraining's rmse: 0.0831549\tvalid_1's rmse: 0.0810009\n",
      "[250]\ttraining's rmse: 0.0830905\tvalid_1's rmse: 0.0809744\n",
      "[275]\ttraining's rmse: 0.0830321\tvalid_1's rmse: 0.0809501\n",
      "[300]\ttraining's rmse: 0.0829726\tvalid_1's rmse: 0.0809275\n",
      "[325]\ttraining's rmse: 0.0829127\tvalid_1's rmse: 0.0809107\n",
      "[350]\ttraining's rmse: 0.0828561\tvalid_1's rmse: 0.0808899\n",
      "[375]\ttraining's rmse: 0.0828113\tvalid_1's rmse: 0.0808777\n",
      "[400]\ttraining's rmse: 0.0827605\tvalid_1's rmse: 0.0808655\n",
      "[425]\ttraining's rmse: 0.0827143\tvalid_1's rmse: 0.0808565\n",
      "[450]\ttraining's rmse: 0.0826683\tvalid_1's rmse: 0.0808453\n",
      "[475]\ttraining's rmse: 0.0826301\tvalid_1's rmse: 0.0808328\n",
      "[500]\ttraining's rmse: 0.0825987\tvalid_1's rmse: 0.08082\n",
      "[525]\ttraining's rmse: 0.0825582\tvalid_1's rmse: 0.0808111\n",
      "[550]\ttraining's rmse: 0.0825182\tvalid_1's rmse: 0.0808099\n",
      "[575]\ttraining's rmse: 0.082482\tvalid_1's rmse: 0.0808134\n",
      "Early stopping, best iteration is:\n",
      "[532]\ttraining's rmse: 0.0825474\tvalid_1's rmse: 0.0808075\n"
     ]
    }
   ],
   "source": [
    "score = 0\n",
    "kf = KFold(n_splits=3)\n",
    "models = []\n",
    "\n",
    "params = {\"objective\": \"regression\", \"boosting\": \"gbdt\", \"num_leaves\": 500, \n",
    "              \"learning_rate\": 0.002515, 'bagging_fraction': 0.8297, \"reg_lambda\": 0.1052, \n",
    "              'reg_alpha':0.1046, \"metric\": \"rmse\", 'max_depth': 10, 'min_child_weight': 27,\n",
    "              'verbose': -1, 'min_split_gain':0.08138 , 'subsample_freq':1, 'sub_feature':  0.4492}\n",
    "for train_index,test_index in kf.split(features):\n",
    "    train_features = features.loc[train_index]\n",
    "    train_target = target.loc[train_index]\n",
    "    test_features = features.loc[test_index]\n",
    "    test_target = target.loc[test_index]\n",
    "    d_training = lgb.Dataset(train_features, label=train_target, \n",
    "                             categorical_feature=categorical_features, free_raw_data=False)\n",
    "    d_test = lgb.Dataset(test_features, label=test_target, \n",
    "                         categorical_feature=categorical_features, free_raw_data=False)\n",
    "    model = lgb.train(params, train_set=d_training, num_boost_round=3000, \n",
    "                      valid_sets=[d_training,d_test], verbose_eval=25, early_stopping_rounds=50)\n",
    "    y_pred_valid = model.predict(test_features)\n",
    "    score += np.sqrt(mean_squared_error(test_target, y_pred_valid)) / 3\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1087.64 MB\n",
      "Memory usage after optimization is: 428.55 MB\n",
      "Decreased by 60.6%\n",
      "Memory usage of dataframe is 1227.28 MB\n",
      "Memory usage after optimization is: 483.58 MB\n",
      "Decreased by 60.6%\n"
     ]
    }
   ],
   "source": [
    "del df_merge\n",
    "gc.collect()\n",
    "df_sub_2016 = reduce_mem_usage(pd.read_csv('./output/final_sub_2016.csv')).drop_duplicates('parcelid')\n",
    "df_sub_2016['year'] = 0\n",
    "results = {}\n",
    "for month in [10,11,12]:\n",
    "    df_sub_2016['month'] = month\n",
    "    for i in models:\n",
    "        if  month not in results.keys():\n",
    "            results[month] = i.predict(df_sub_2016[list(features.columns)], \n",
    "                                       num_iteration=i.best_iteration) / len(models)\n",
    "        else:\n",
    "            results[month] += i.predict(df_sub_2016[list(features.columns)], \n",
    "                                        num_iteration=i.best_iteration) / len(models)\n",
    "for i in results.keys():\n",
    "    df_sub_2016[str(i)] = results[i]\n",
    "df_sub = pd.read_csv('./Resources/sample_submission.csv')\n",
    "df_sub = df_sub.rename(columns = {'ParcelId': 'parcelid'})\n",
    "df_sub = pd.merge(df_sub[['parcelid']], df_sub_2016[['parcelid', '10', '11', '12']].drop_duplicates('parcelid'),\n",
    "                  how = 'left', on = 'parcelid')\n",
    "df_sub = df_sub.rename(columns = {'10': '201610', '11': '201611', '12': '201612'}).drop_duplicates('parcelid')\n",
    "del df_sub_2016\n",
    "gc.collect()\n",
    "df_sub_2017 = reduce_mem_usage(pd.read_csv('./output/final_sub_2017.csv'))\n",
    "df_sub_2017['year'] = 1\n",
    "results = {}\n",
    "for month in [10,11,12]:\n",
    "    df_sub_2017['month'] = month\n",
    "    for i in models:\n",
    "        if  month not in results.keys():\n",
    "            results[month] = i.predict(df_sub_2017[list(features.columns)], \n",
    "                                       num_iteration=i.best_iteration) / len(models)\n",
    "        else:\n",
    "            results[month] += i.predict(df_sub_2017[list(features.columns)], \n",
    "                                        num_iteration=i.best_iteration) / len(models)\n",
    "for i in results.keys():\n",
    "    df_sub_2017[str(i)] = results[i]\n",
    "df_sub = pd.merge(df_sub[['parcelid', '201610', '201611', '201612']], \n",
    "                  df_sub_2017[['parcelid', '10', '11', '12']].drop_duplicates('parcelid'), \n",
    "                  how = 'left', on = 'parcelid')\n",
    "df_sub = df_sub.rename(columns = {'10': '201710', '11': '201711', '12': '201712'})\n",
    "del df_sub_2017\n",
    "gc.collect()\n",
    "df_sub.to_csv('./output/sample_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parcelid</th>\n",
       "      <th>201610</th>\n",
       "      <th>201611</th>\n",
       "      <th>201612</th>\n",
       "      <th>201710</th>\n",
       "      <th>201711</th>\n",
       "      <th>201712</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10754147</td>\n",
       "      <td>-0.008423</td>\n",
       "      <td>-0.008376</td>\n",
       "      <td>-0.008376</td>\n",
       "      <td>-0.006210</td>\n",
       "      <td>-0.006163</td>\n",
       "      <td>-0.006163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10759547</td>\n",
       "      <td>-0.002626</td>\n",
       "      <td>-0.002611</td>\n",
       "      <td>-0.002611</td>\n",
       "      <td>-0.000116</td>\n",
       "      <td>-0.000100</td>\n",
       "      <td>-0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10843547</td>\n",
       "      <td>0.004614</td>\n",
       "      <td>0.004620</td>\n",
       "      <td>0.004620</td>\n",
       "      <td>0.002415</td>\n",
       "      <td>0.002457</td>\n",
       "      <td>0.002457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10859147</td>\n",
       "      <td>0.013256</td>\n",
       "      <td>0.013272</td>\n",
       "      <td>0.013272</td>\n",
       "      <td>0.012729</td>\n",
       "      <td>0.012744</td>\n",
       "      <td>0.012744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10879947</td>\n",
       "      <td>0.009906</td>\n",
       "      <td>0.009922</td>\n",
       "      <td>0.009922</td>\n",
       "      <td>0.009583</td>\n",
       "      <td>0.009599</td>\n",
       "      <td>0.009599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2985212</th>\n",
       "      <td>168176230</td>\n",
       "      <td>0.010511</td>\n",
       "      <td>0.010558</td>\n",
       "      <td>0.010558</td>\n",
       "      <td>0.016145</td>\n",
       "      <td>0.016151</td>\n",
       "      <td>0.016151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2985213</th>\n",
       "      <td>14273630</td>\n",
       "      <td>0.010378</td>\n",
       "      <td>0.010425</td>\n",
       "      <td>0.010425</td>\n",
       "      <td>0.017108</td>\n",
       "      <td>0.017108</td>\n",
       "      <td>0.017108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2985214</th>\n",
       "      <td>168040630</td>\n",
       "      <td>0.010511</td>\n",
       "      <td>0.010558</td>\n",
       "      <td>0.010558</td>\n",
       "      <td>0.009141</td>\n",
       "      <td>0.009141</td>\n",
       "      <td>0.009141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2985215</th>\n",
       "      <td>168040830</td>\n",
       "      <td>0.010374</td>\n",
       "      <td>0.010421</td>\n",
       "      <td>0.010421</td>\n",
       "      <td>0.045609</td>\n",
       "      <td>0.045645</td>\n",
       "      <td>0.045645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2985216</th>\n",
       "      <td>168040430</td>\n",
       "      <td>0.010374</td>\n",
       "      <td>0.010421</td>\n",
       "      <td>0.010421</td>\n",
       "      <td>0.009581</td>\n",
       "      <td>0.009581</td>\n",
       "      <td>0.009581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2985217 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          parcelid    201610    201611    201612    201710    201711    201712\n",
       "0         10754147 -0.008423 -0.008376 -0.008376 -0.006210 -0.006163 -0.006163\n",
       "1         10759547 -0.002626 -0.002611 -0.002611 -0.000116 -0.000100 -0.000100\n",
       "2         10843547  0.004614  0.004620  0.004620  0.002415  0.002457  0.002457\n",
       "3         10859147  0.013256  0.013272  0.013272  0.012729  0.012744  0.012744\n",
       "4         10879947  0.009906  0.009922  0.009922  0.009583  0.009599  0.009599\n",
       "...            ...       ...       ...       ...       ...       ...       ...\n",
       "2985212  168176230  0.010511  0.010558  0.010558  0.016145  0.016151  0.016151\n",
       "2985213   14273630  0.010378  0.010425  0.010425  0.017108  0.017108  0.017108\n",
       "2985214  168040630  0.010511  0.010558  0.010558  0.009141  0.009141  0.009141\n",
       "2985215  168040830  0.010374  0.010421  0.010421  0.045609  0.045645  0.045645\n",
       "2985216  168040430  0.010374  0.010421  0.010421  0.009581  0.009581  0.009581\n",
       "\n",
       "[2985217 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
